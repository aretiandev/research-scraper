{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio\n",
    "import ast\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1b193e-f76d-4d81-ae23-9fac454cc603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    print(\"Retrieving number of URLs to scrape:\", end=\" \")\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_items = r.html.find('div.discovery-result-pagination ul.pagination li')\n",
    "    max_pages_str = pagination_items[-2].text.split(\"\\n\")[0].strip().replace('.','').replace(',','')\n",
    "    max_pages = int(max_pages_str)\n",
    "    print(f\"{max_pages:,d}.\")\n",
    "    return max_pages\n",
    "\n",
    "\n",
    "async def retry_url(s, url, attempts=10):\n",
    "    \"\"\"Retry fetching URL a given number of times.\"\"\"\n",
    "    for _ in range(attempts):\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            pass\n",
    "    else:\n",
    "        r = {}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd0481-5196-403b-b52a-960310684044",
   "metadata": {},
   "source": [
    "## Helper for author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c9d361-6e80-46ce-a5e7-46640d35a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author(s, url, item='name', attempts=10):\n",
    "    if item == 'name':\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        scraped_objects = r.html.find(selector)\n",
    "        try:\n",
    "            result = scraped_objects[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'id':\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        scraped_objects = r.html.find(selector)\n",
    "        try:\n",
    "            result = scraped_objects[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'affiliation':\n",
    "        url = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        selector = 'table.table tr'\n",
    "        rows = r.html.find(selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            departments = []\n",
    "            institutions = []\n",
    "            for row in rows:\n",
    "                columns = row.find('td')\n",
    "                departments.append(columns[0].text)\n",
    "                institutions.append(columns[1].text)\n",
    "            result['department'] = departments\n",
    "            result['institution'] = institutions\n",
    "        except: # rows object is empty\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        selector = 'table.table tr'\n",
    "        rows = r.html.find(selector)\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            project = row.find('td a')[0].attrs['href']\n",
    "            result.append(project)\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url = url + '/orgs.html?onlytab=true'\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        selector = 'table.table tr'\n",
    "        rows = r.html.find(selector)\n",
    "        result = []\n",
    "        for row in rows:\n",
    "            group = row.find('td a')[0].attrs['href']\n",
    "            result.append(group)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26147aae-bc95-47c9-9c71-d0510bb763dc",
   "metadata": {},
   "source": [
    "## Helper for project and group pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edd7f5a-9683-4e1b-b1d3-4a0b419f99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_project(s, url, tab='information', attempts=10):\n",
    "    if tab == 'information':\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        selector = 'div#collapseOneprimarydata'\n",
    "        table = r.html.find(selector, first=True)\n",
    "        \n",
    "        project = {}\n",
    "        \n",
    "        attributes = {\n",
    "            '#titleDiv' : 'title', \n",
    "            '#oficialcodeDiv': 'official code',\n",
    "            '#programDiv': 'program',\n",
    "            '#startdateDiv': 'start date',\n",
    "            '#expdateDiv': 'end date',\n",
    "            '#universityDiv': 'institution',\n",
    "        }\n",
    "        \n",
    "        for selector in attributes.keys():\n",
    "            try:\n",
    "                attribute_value = table.find(selector, first=True).text\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            \n",
    "            attribute       = attributes[selector]\n",
    "            project[attribute] = attribute_value\n",
    "            \n",
    "    elif tab == 'researchers':\n",
    "        url = url + '/researchersprj.html?onlytab=true'\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        selector = 'table.table'\n",
    "        tables = r.html.find(selector)\n",
    "        \n",
    "        project = {}\n",
    "        \n",
    "        # Principal researchers\n",
    "        rows = tables[0].find('tr')\n",
    "        principal_names = []\n",
    "        principal_ids = []\n",
    "        \n",
    "        for row in rows[1:]:\n",
    "            name_object = row.find('td', first=True)\n",
    "            principal_names.append(name_object.text)\n",
    "            try:\n",
    "                orcid = name_object.find('a', first=True).attrs['href'][7:]\n",
    "                principal_ids.append(orcid)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        project['principal names'] = principal_names\n",
    "        project['principal ids'] = principal_ids\n",
    "        \n",
    "        # Researchers\n",
    "        try:\n",
    "            rows = tables[1].find('tr')\n",
    "            res_names = []\n",
    "            res_ids = []\n",
    "\n",
    "            for row in rows[1:]:\n",
    "                name_object = row.find('td', first=True)\n",
    "                res_names.append(name_object.text)\n",
    "                try:\n",
    "                    orcid = name_object.find('a', first=True).attrs['href'][7:]\n",
    "                    res_ids.append(orcid)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            project['researcher names'] = res_names\n",
    "            project['researcher ids'] = res_ids\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                \n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e3429a4-2f01-4eab-bc2d-8aa2c771def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_group(s, url, tab='information', attempts=10):\n",
    "    if tab == 'information':\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        selector = 'div#collapseOneorgcard'\n",
    "        table = r.html.find(selector, first=True)\n",
    "        \n",
    "        group = {}\n",
    "        \n",
    "        attributes = {\n",
    "            '#nameDiv' : 'name', \n",
    "            '#acronymDiv': 'acronym',\n",
    "            '#sgrDiv': 'sgr',\n",
    "            '#urlDiv': 'url',\n",
    "            '#universityDiv': 'institution',\n",
    "            # '#startdateDiv': 'start date',\n",
    "            # '#expdateDiv': 'end date',\n",
    "        }\n",
    "        \n",
    "        for selector in attributes.keys():\n",
    "            try:\n",
    "                attribute_value = table.find(selector, first=True).text\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            \n",
    "            attribute       = attributes[selector]\n",
    "            group[attribute] = attribute_value\n",
    "            \n",
    "            \n",
    "    elif tab == 'researchers':\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        # Get next tab url\n",
    "        selector = 'div#tabs ul li'\n",
    "        next_tab_url = r.html.find(selector)[1].find('a', first=True).attrs['href']\n",
    "        url_root = 'https://portalrecerca.csuc.cat'\n",
    "        next_tab_url = url_root + next_tab_url\n",
    "                                                                \n",
    "        r = await retry_url(s, next_tab_url, attempts)\n",
    "        \n",
    "        selector = 'table.table'\n",
    "        tables = r.html.find(selector)\n",
    "        \n",
    "        group = {}\n",
    "        \n",
    "        # Principal researchers\n",
    "        rows = tables[0].find('tr')\n",
    "        principal_names = []\n",
    "        principal_ids = []\n",
    "        \n",
    "        for row in rows[1:]:\n",
    "            name_object = row.find('td', first=True)\n",
    "            principal_names.append(name_object.text)\n",
    "            try:\n",
    "                orcid = name_object.find('a', first=True).attrs['href'][7:]\n",
    "                principal_ids.append(orcid)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        group['principal names'] = principal_names\n",
    "        group['principal ids'] = principal_ids\n",
    "        \n",
    "        # Researchers\n",
    "        try:\n",
    "            rows = tables[1].find('tr')\n",
    "            res_names = []\n",
    "            res_ids = []\n",
    "\n",
    "            for row in rows[1:]:\n",
    "                name_object = row.find('td', first=True)\n",
    "                res_names.append(name_object.text)\n",
    "                try:\n",
    "                    orcid = name_object.find('a', first=True).attrs['href'][7:]\n",
    "                    res_ids.append(orcid)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            group['researcher names'] = res_names\n",
    "            group['researcher ids'] = res_ids\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "                \n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13964115-f898-4541-8885-e6fbbc9b1bc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrape single URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e283f3-3b91-4613-b08e-d8c8ff060ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='authors', attempts=10):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items: [paper_links, papers, author_links, authors, project_links, projects, group_links, groups]\n",
    "    \"\"\"\n",
    "    \n",
    "    if items == 'authors':\n",
    "        result = await asyncio.gather(\n",
    "                scrape_author(s, url, 'name'),\n",
    "                scrape_author(s, url, 'id'),\n",
    "                scrape_author(s, url, 'affiliation'),\n",
    "                scrape_author(s, url, 'projects'),\n",
    "                scrape_author(s, url, 'groups')\n",
    "            )\n",
    "        \n",
    "        author = {}\n",
    "        author['name'] = result[0]\n",
    "        author['id'] = result[1]\n",
    "        try:\n",
    "            author['department'] = result[2]['department']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            author['institution'] = result[2]['institution']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        author['projects'] = result[3]\n",
    "        author['groups'] = result[4]\n",
    "\n",
    "        return author\n",
    "        \n",
    "        \n",
    "    elif items == 'projects':    \n",
    "        result = await asyncio.gather(\n",
    "                scrape_project(s, url, tab='information', attempts=attempts),\n",
    "                scrape_project(s, url, tab='researchers', attempts=attempts),\n",
    "            )\n",
    "\n",
    "        project = {}\n",
    "        for d in result:\n",
    "            project.update(d)\n",
    "            \n",
    "        project['url']    = url\n",
    "        project['url_id'] = url[31:].split('?')[0]\n",
    "        \n",
    "        return project\n",
    "    \n",
    "    \n",
    "    elif items == 'groups':    \n",
    "        result = await asyncio.gather(\n",
    "                scrape_group(s, url, tab='information', attempts=attempts),\n",
    "                scrape_group(s, url, tab='researchers', attempts=attempts),\n",
    "            )\n",
    "\n",
    "        group = {}\n",
    "        for d in result:\n",
    "            group.update(d)\n",
    "            \n",
    "        group['url']    = url\n",
    "        group['url_id'] = url[31:].split('?')[0]\n",
    "        \n",
    "        return group\n",
    "    \n",
    "        \n",
    "    elif items == 'papers':\n",
    "        paper = {}\n",
    "        paper['url']         = url\n",
    "        paper['url_id']      = url[31:].split('?')[0]\n",
    "        \n",
    "        not_found_msg = 'No ha estat possible trobar el que esteu buscant'\n",
    "        \n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        try:\n",
    "            title = r.html.find('title', first=True)\n",
    "            table = r.html.find('table.table', first=True)\n",
    "            rows = table.find('tr')\n",
    "        except:\n",
    "            paper['status code'] = r.status_code\n",
    "            try:\n",
    "                if not_found_msg in title.text:\n",
    "                    paper['status description'] = 'Title: not found'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return paper\n",
    "            \n",
    "        for row in rows:\n",
    "            # Get columns in row\n",
    "            columns = row.find('td')\n",
    "            \n",
    "            # Skip if empty row\n",
    "            if len(columns) == 0:\n",
    "                continue\n",
    "            \n",
    "            attributes = {\n",
    "                'dc.contributor.authors' : 'authors', \n",
    "                'dc.date.issued'         : 'date',\n",
    "                'dc.publisher'           : 'publisher',\n",
    "                'dc.identifier.citation' : 'citation',\n",
    "                'dc.identifier.issn'     : 'issn',\n",
    "                'dc.identifier.uri'      : 'uri',\n",
    "                'dc.identifier.isbn'     : 'isbn',\n",
    "                'dc.relation.ispartof'   : 'published_in',\n",
    "                'dc.title'               : 'title',\n",
    "                'dc.type'                : 'type',\n",
    "                'dc.identifier.doi'      : 'doi', \n",
    "                'dc.identifier.sourceid' : 'sourceid',\n",
    "                'dc.identifier.sourceref': 'sourceref',\n",
    "                'Appears in Collections:': 'appears_in_collections'}\n",
    "\n",
    "            for row_index in attributes.keys():\n",
    "                if columns[0].text == row_index:\n",
    "                    paper[attributes[row_index]] = columns[1].text\n",
    "\n",
    "        # Get authors ORCid'ss\n",
    "        simple_url = url.split('?')[0] + '?mode=simple'\n",
    "        \n",
    "        r = await retry_url(s, simple_url, attempts)\n",
    "        \n",
    "        try:\n",
    "            title = r.html.find('title', first=True)\n",
    "            table = r.html.find('table.table', first=True)\n",
    "            rows = table.find('tr')\n",
    "            authors = r.html.find('table.table a.authority.author')\n",
    "            author_hrefs = []\n",
    "            for author in authors:\n",
    "                href = author.attrs['href']\n",
    "                if href[:7] == '/orcid/':\n",
    "                    href = href[7:]\n",
    "                author_hrefs.append(href)\n",
    "        except:\n",
    "            paper['status code'] = r.status_code\n",
    "            try:\n",
    "                if not_found_msg in title.text:\n",
    "                    paper['status description'] = 'Title: not found'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return paper\n",
    "            \n",
    "        paper['orcids'] = author_hrefs\n",
    "        \n",
    "        return paper\n",
    "            \n",
    "        \n",
    "    else: # paper_links, author_links, project_links or group_links\n",
    "        r = await retry_url(s, url, attempts)\n",
    "        \n",
    "        try:\n",
    "            table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "            rows = table.find('tr')\n",
    "            \n",
    "            result = []\n",
    "\n",
    "            for row in rows:\n",
    "                # Get columns in row\n",
    "                columns = row.find('td')\n",
    "\n",
    "                # Skip if empty row\n",
    "                if len(columns) == 0:\n",
    "                    continue\n",
    "\n",
    "                if items in ['paper_links', 'project_links']:\n",
    "                    scrape_item = columns[1].find('a')[0].attrs['href']\n",
    "                elif items in ['author_links', 'group_links']:\n",
    "                    scrape_item = columns[0].find('a')[0].attrs['href']\n",
    "\n",
    "                # Append to results_list\n",
    "                result.append(scrape_item)\n",
    "            \n",
    "        except:\n",
    "            result = []\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a07b54-6be2-43f7-81fb-4186b46acc55",
   "metadata": {},
   "source": [
    "## Main Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91abdf87-81a7-4065-91d2-b36c9177bbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape(\n",
    "    items='authors', \n",
    "    urls=None, \n",
    "    start_pos=0, \n",
    "    n_pages=None, \n",
    "    batch_size=None, \n",
    "    out_file=None):\n",
    "    \"\"\"\n",
    "    Main entry function to scrape Portal de la Reserca.\n",
    "    Options:\n",
    "        items: [authors, papers, author_links, paper_links]\n",
    "        urls: list of urls. Not needed if items in ['author_links', 'paper_links'].\n",
    "        start_pos: starting position.\n",
    "        n_pages: max pages to scrape.\n",
    "        batch_size: batch size.\n",
    "        out_file: output file.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "    if out_file:\n",
    "        print(f\"Saving results to {out_file}.\")\n",
    "    \n",
    "\n",
    "    # Get list of URLs to scrape hyperlinks\n",
    "    if items in ['author_links', 'paper_links', 'project_links', 'group_links']:\n",
    "        \n",
    "        if not urls:\n",
    "            url_template =                                        \\\n",
    "                'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "                'query='                                        + \\\n",
    "                '&location={location}'                          + \\\n",
    "                '&filter_field_1={filter_field_1}'              + \\\n",
    "                    '&filter_type_1={filter_type_1}'            + \\\n",
    "                    '&filter_value_1={filter_value_1}'          + \\\n",
    "                '&filter_field_2={filter_field_2}'              + \\\n",
    "                    '&filter_type_2={filter_type_2}'            + \\\n",
    "                    '&filter_value_2={filter_value_2}'          + \\\n",
    "                '&sort_by={sort_by}'                            + \\\n",
    "                    '&order={order}'                            + \\\n",
    "                '&rpp={rpp}'                                    + \\\n",
    "                '&etal=0'                                       + \\\n",
    "                '&start='\n",
    "            \n",
    "            if items == 'author_links':\n",
    "                search_fields = {\n",
    "                    'location'      : 'crisrp',\n",
    "                    'filter_field_1': 'resourcetype',\n",
    "                    'filter_type_1' : 'equals',\n",
    "                    'filter_value_1': 'Researchers',\n",
    "                    'filter_field_2': '',\n",
    "                    'filter_type_2' : '',\n",
    "                    'filter_value_2': '',\n",
    "                    'sort_by'       : 'crisrp.fullName_sort',\n",
    "                    'order'         : 'asc',\n",
    "                    'rpp'           : '300'\n",
    "                }\n",
    "                \n",
    "            elif items == 'paper_links':\n",
    "                search_fields = {\n",
    "                    'location'      : 'publications',\n",
    "                    'filter_field_1': 'resourcetype',\n",
    "                    'filter_type_1' : 'equals',\n",
    "                    'filter_value_1': 'Items',\n",
    "                    'filter_field_2': 'itemtype',\n",
    "                    'filter_type_2' : 'notequals',\n",
    "                    'filter_value_2': 'Phd+Thesis',\n",
    "                    'sort_by'       : 'dc.contributor.authors_sort',\n",
    "                    'order'         : 'asc',\n",
    "                    'rpp'           : '300'\n",
    "                }\n",
    "                \n",
    "            elif items == 'project_links':\n",
    "                search_fields = {\n",
    "                    'location'      : 'crisproject',\n",
    "                    'filter_field_1': 'resourcetype',\n",
    "                    'filter_type_1' : 'equals',\n",
    "                    'filter_value_1': 'Projects',\n",
    "                    'filter_field_2': '',\n",
    "                    'filter_type_2' : '',\n",
    "                    'filter_value_2': '',\n",
    "                    'sort_by'       : 'crisrp.title_sort',\n",
    "                    'order'         : 'asc',\n",
    "                    'rpp'           : '300'\n",
    "                }\n",
    "\n",
    "            elif items == 'group_links':\n",
    "                search_fields = {\n",
    "                    'location'      : 'crisou',\n",
    "                    'filter_field_1': 'resourcetype',\n",
    "                    'filter_type_1' : 'equals',\n",
    "                    'filter_value_1': 'OrgUnits',\n",
    "                    'filter_field_2': '',\n",
    "                    'filter_type_2' : '',\n",
    "                    'filter_value_2': '',\n",
    "                    'sort_by'       : 'crisou.name_sort',\n",
    "                    'order'         : 'asc',\n",
    "                    'rpp'           : '300'\n",
    "                }\n",
    "            \n",
    "            url_root = url_template.format(**search_fields)\n",
    "\n",
    "            if not n_pages:\n",
    "                n_pages = get_max_pages(url_root + '0')\n",
    "        \n",
    "            urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "                \n",
    "\n",
    "    if not batch_size:\n",
    "        batch_size = len(urls)\n",
    "\n",
    "    batch_urls = [urls[i:i+batch_size] for i in range(start_pos, len(urls), batch_size)]\n",
    "    \n",
    "    print(f\"Scraping {items} from {len(urls)-start_pos:,d} URLs in {len(batch_urls):,d} batches of {batch_size:,d}, starting at {start_pos:,d}.\")\n",
    "\n",
    "    if out_file:\n",
    "        result_df = pd.DataFrame()\n",
    "\n",
    "    result = []\n",
    "    for i, batch in enumerate(batch_urls):\n",
    "\n",
    "        s = AsyncHTMLSession()\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        tasks = (scrape_url(s, url, items=items) for url in batch)\n",
    "        batch_result = await asyncio.gather(*tasks)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # Flatten result\n",
    "        if items in ['author_links', 'paper_links', 'project_links', 'group_links']:\n",
    "            batch_result = [i for sublist in batch_result for i in sublist]\n",
    "\n",
    "        result.extend(batch_result)\n",
    "\n",
    "        if out_file:\n",
    "            result_df = result_df.append(batch_result, ignore_index=True)\n",
    "            result_df.to_csv(out_file, index=None)\n",
    "\n",
    "        # Print estimated time left\n",
    "        seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "        m, s = divmod(seconds_left, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        \n",
    "        print(\n",
    "            f\"Progress: {(i+1)/len(batch_urls)*100:.0f}% ({i+1}/{len(batch_urls):,d}). URLs: {i*batch_size}-{(i+1)*batch_size-1}. \" +\n",
    "            f\"Batch time: {t2-t1:.2f}s. Time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\"\\r\")\n",
    "        \n",
    "    print(\"\\nDone.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb4382-8dbc-4b12-93d5-e4700342dec4",
   "metadata": {},
   "source": [
    "# Run Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nodes: Scrape authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "### Get links to author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d990062-5b71-4cb7-9a9b-c557b330d97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping author_links from Portal de la Reserca.\n",
      "Saving results to ./data/author_urls_20220301.csv.\n",
      "Retrieving number of URLs to scrape: 64.\n",
      "Scraping author_links from 64 URLs in 4 batches of 20, starting at 0.\n",
      "Progress: 100% (4/4). URLs: 60-79. Batch time: 33.41s. Time left: 0h0m33s.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "items = 'author_links'\n",
    "batch_size = 20\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/author_urls_{date_today}.csv'\n",
    "author_urls = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "### Scrape author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e02e1f-7c70-4e46-9928-c1121591c2db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping authors from Portal de la Reserca.\n",
      "Saving results to ./data/nodes_20220301.csv.\n",
      "Scraping authors from 19,050 URLs in 191 batches of 100, starting at 0.\n",
      "Progress: 4% (7/191). URLs: 600-699. Batch time: 21.81s. Time left: 1h7m15s..\r"
     ]
    }
   ],
   "source": [
    "# Build urls\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "author_urls = pd.read_csv(f'./data/author_urls_{date_today}.csv')\n",
    "author_urls = list(author_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in author_urls]\n",
    "\n",
    "# Get author data in batch\n",
    "items = 'authors'\n",
    "batch_size = 100\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/nodes_{date_today}.csv'\n",
    "author_data = await scrape(items=items, urls=urls, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41d042-1de5-43de-b1c3-91820088595f",
   "metadata": {},
   "source": [
    "## Nodes: Scrape projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5ef93-d776-4422-918d-e6ee0ccf6d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get links to projects from Portal de la Reserca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79c1f1-f104-4a9d-aa97-a3051cfc2971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = 'project_links'\n",
    "batch_size = 10\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/project_links_{date_today}.csv'\n",
    "project_urls = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3317e1-2fe9-446a-bd57-3ac019298c62",
   "metadata": {},
   "source": [
    "### Alternative: get links to projects from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e300af-11fb-47be-bcad-c05ee4c37dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_df = pd.read_csv(\"./data/nodes.csv\")\n",
    "# projects = set()\n",
    "# for projects_string in nodes_df['projects']:\n",
    "#     projects_list = ast.literal_eval(projects_string)\n",
    "#     projects.update(projects_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064fb1e-9a01-442b-950a-4f6afc04d7d1",
   "metadata": {},
   "source": [
    "### Build URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d50d42-85cd-49e2-bb08-c318609f67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_urls = pd.read_csv(f'./data/project_links_{date_today}.csv')\n",
    "project_urls = list(project_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in project_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab5ab93-f4e5-4793-bd4e-4605dc5c41f0",
   "metadata": {},
   "source": [
    "### Scrape projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4caea71-e5b2-48e4-bac8-12f62f58d3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = 'projects'\n",
    "batch_size = 10\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/projects_{date_today}.csv'\n",
    "projects = await scrape(items=items, urls=urls, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c942a8-d8be-4082-9312-063ad719669e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nodes: Scrape groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb90c2-faad-43ed-a8e6-424b1daaad53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get links to groups from Portal de la Reserca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ca8eb-610d-4d14-a7a4-6ff13ef1cd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = 'group_links'\n",
    "batch_size = 10\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/group_links_{date_today}.csv'\n",
    "group_urls = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7377314-ed1d-4760-8013-839a2a73285d",
   "metadata": {},
   "source": [
    "### Alternative: get links to groups from nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a24889-3744-4d31-b70d-f7dc3a3f89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_df = pd.read_csv(\"./data/nodes.csv\")\n",
    "# groups = set()\n",
    "# for groups_string in nodes_df['groups']:\n",
    "#     groups_list = ast.literal_eval(groups_string)\n",
    "#     groups.update(groups_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b54832-14e1-421e-9040-d7b2cde4fcd9",
   "metadata": {},
   "source": [
    "### Build URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c5dfa-d26f-4a81-917f-4bcf28d3bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_urls = pd.read_csv(f'./data/group_links_{date_today}.csv')\n",
    "group_urls = list(group_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in group_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17297a-32d4-4c43-a0b4-810dd9cde7b1",
   "metadata": {},
   "source": [
    "###  Scrape groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc09d0-1a02-4515-ab4b-4ec8275e6b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = 'groups'\n",
    "batch_size = 10\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/groups_{date_today}.csv'\n",
    "groups = await scrape(items=items, urls=urls, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940415cc-f81d-4c3e-a774-666723f2c783",
   "metadata": {},
   "source": [
    "## Edgelist: scrape publications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e484bc-cc8f-4c23-a346-700c8057a180",
   "metadata": {},
   "source": [
    "### Get links to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea2dbe-687a-435d-832c-5ebc1e8ecc79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = 'paper_links'\n",
    "batch_size = 10\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/paper_links_{date_today}.csv'\n",
    "\n",
    "paper_urls = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe64e77-08d4-4bde-b1ac-1d017c5f5217",
   "metadata": {},
   "source": [
    "### Get coauthors in paper pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30836817-ad61-4766-9c75-6050400d8e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build urls\n",
    "paper_urls = pd.read_csv(f'./data/paper_links_{date_today}.csv')\n",
    "paper_urls = list(paper_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url + '?mode=full' for url in paper_urls]\n",
    "\n",
    "# Run in batch\n",
    "items = 'papers'\n",
    "batch_size = 10\n",
    "start_pos = 56040 # old starting position with problems\n",
    "# start_pos = 0\n",
    "date_today = date.today().strftime(\"%Y%m%d\")\n",
    "out_file = f'./data/papers_{date_today}.csv'\n",
    "\n",
    "papers = await scrape(items=items, urls=urls, start_pos=start_pos, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c8900-e322-4924-ab6c-e1f83947da81",
   "metadata": {},
   "source": [
    "# For script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8804152-56e7-4368-8dca-e2c53dda814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) == 2:\n",
    "        items = sys.argv[1]\n",
    "        batch_size = 20\n",
    "    elif len(sys.argv) == 3:\n",
    "        items = sys.argv[1]\n",
    "        batch_size = sys.argv[2]\n",
    "    elif len(sys.argv) > 3:\n",
    "        print(\"Too many arguments. Choose argument 1: author_links, paper_links, authors, papers, projects, groups. Choose argument 2: batch size.\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Choose argument 1: author_links, paper_links, authors, papers, projects, groups. Choose argument 2: batch size.\")\n",
    "        return 1\n",
    "\n",
    "    date_today = date.today().strftime(\"%Y%m%d\")\n",
    "    url_root = 'https://portalrecerca.csuc.cat'\n",
    "    \n",
    "    if items == 'author_links':\n",
    "        urls = None\n",
    "        out_file = f'./data/author_urls_{date_today}.csv'\n",
    "        \n",
    "    elif items == 'authors':\n",
    "        author_urls = pd.read_csv(f'./data/author_urls_{date_today}.csv')\n",
    "        author_urls = list(author_urls['author_urls'])\n",
    "        urls = [url_root + url for url in author_urls]\n",
    "        out_file = f'./data/nodes_{date_today}.csv'\n",
    "        \n",
    "    elif items == 'project_links':\n",
    "        urls=None\n",
    "        out_file = f'./data/project_links_{date_today}.csv'\n",
    "        \n",
    "    elif items == 'projects:\n",
    "        project_urls = pd.read_csv(f'./data/project_links_{date_today}.csv')\n",
    "        project_urls = list(project_urls['0'])\n",
    "        urls = [url_root + url for url in project_urls]\n",
    "        out_file = f'./data/projects_{date_today}.csv'\n",
    "    \n",
    "    elif items == 'group_links':\n",
    "        urls=None\n",
    "        out_file = f'./data/group_links_{date_today}.csv'\n",
    "        \n",
    "    elif items == 'groups':\n",
    "        group_urls = pd.read_csv(f'./data/group_links_{date_today}.csv')\n",
    "        group_urls = list(group_urls['0'])\n",
    "        urls = [url_root + url for url in group_urls]\n",
    "        out_file = f'./data/groups_{date_today}.csv'\n",
    "    \n",
    "    elif items == 'paper_links':\n",
    "        urls = None\n",
    "        out_file = f'./data/paper_links_{date_today}.csv'\n",
    "        \n",
    "    elif items == 'papers':\n",
    "        paper_urls = pd.read_csv(f'./data/paper_links_{date_today}.csv')\n",
    "        paper_urls = list(paper_urls['0'])\n",
    "        urls = [url_root + url + '?mode=full' for url in paper_urls]\n",
    "        out_file = f'./data/papers_{date_today}.csv'\n",
    "\n",
    "    asyncio.run(scrape(items=items, urls=urls, batch_size=batch_size, out_file=out_file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
