{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## Helper Functions for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0623840-a0bb-4fa0-9045-7a5177cccff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "    \"\"\"\n",
    "    # print(f\"Scraping url: {url}\")\n",
    "    r = await s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for row in rows:\n",
    "        # Get columns in row\n",
    "        columns = row.find('td')\n",
    "        # Skip if empty row\n",
    "        if len(columns) == 0:\n",
    "            continue\n",
    "            \n",
    "        if items == 'paper_links':\n",
    "            # Get paper link\n",
    "            paper_link = columns[1].find('a')[0].attrs['href']\n",
    "            scrape_item = paper_link\n",
    "            \n",
    "        elif items == 'author_links':\n",
    "            # Get paper link\n",
    "            author_link = columns[0].find('a')[0].attrs['href']\n",
    "            scrape_item = author_link\n",
    "            \n",
    "        elif items == 'papers':\n",
    "            # Get paper data\n",
    "            paper_date    = columns[0].text\n",
    "            paper_title   = columns[1].text\n",
    "            paper_authors = columns[2].text\n",
    "            paper_type    = columns[3].text\n",
    "\n",
    "            paper = {}\n",
    "            paper['date'] = paper_date\n",
    "            paper['title'] = paper_title\n",
    "            paper['type'] = paper_type\n",
    "\n",
    "            paper_authors_list= paper_authors.split(';')\n",
    "            for i, author in enumerate(paper_authors_list):\n",
    "                paper[f\"author_{i}\"] = author\n",
    "\n",
    "            scrape_item = paper\n",
    "\n",
    "        elif items == 'authors':\n",
    "            # Get author data\n",
    "            author_name = columns[0].text\n",
    "            author_last = author_name.split(',')[0]\n",
    "\n",
    "            try:\n",
    "                author_first = author_name.split(',')[1]\n",
    "            except IndexError:  # If there is no comma in the name\n",
    "                author_first = ''\n",
    "\n",
    "            author_inst = columns[1].text\n",
    "\n",
    "            author_dict = {\n",
    "                'Last Name': author_last,\n",
    "                'First Name': author_first,\n",
    "                'Institution': author_inst,\n",
    "                }\n",
    "\n",
    "            scrape_item = author_dict\n",
    "            \n",
    "        # Append to paper links list\n",
    "        result_list.append(scrape_item)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "\n",
    "async def main(urls, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async main loop. Run withn await main(urls) in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (scrape_url(s, url, items) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_box = r.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)\n",
    "\n",
    "async def scrape(items='author_links', n_pages=None):\n",
    "    \"\"\"\n",
    "    Scrape Portal de la Reserca.\n",
    "    Options:\n",
    "        items   = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "        n_pages = Number of pages to scrape.\n",
    "    \"\"\"\n",
    "    print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "    \n",
    "    # Url root for authors\n",
    "    url_root = \\\n",
    "        'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "        'query='                                        + \\\n",
    "        '&location=crisrp'                              + \\\n",
    "        '&filter_field_1=resourcetype'                  + \\\n",
    "            '&filter_type_1=equals'                     + \\\n",
    "            '&filter_value_1=Researchers'               + \\\n",
    "        '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "            '&order=asc'                                + \\\n",
    "        '&rpp=300'                                      + \\\n",
    "        '&etal=0'                                       + \\\n",
    "        '&start='\n",
    "\n",
    "    if not n_pages:\n",
    "        print(\"Calculating number of pages to scrape.\")\n",
    "        max_pages = get_max_pages(url_root + '0')\n",
    "        n_pages = max_pages\n",
    "\n",
    "    urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "\n",
    "    items_to_scrape = 'author_links'\n",
    "\n",
    "    print(f\"Scraping {len(urls)} URLs...\")\n",
    "    t1 = time.perf_counter()\n",
    "    result = await main(urls, items=items_to_scrape)\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    # Gather all results into single list\n",
    "    full_list = [href for sublist in result for href in sublist]\n",
    "    \n",
    "    print(f\"Scraped {len(full_list)} items in {t2-t1:.2f} seconds.\")\n",
    "    \n",
    "    return full_list\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b8973-a368-4e42-b12f-a8efea8add6d",
   "metadata": {},
   "source": [
    "## Helper functions for Author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ed5c6-1e9e-4d08-95fb-eb1ed7c0d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author_tab(s, url, selector):\n",
    "    while True:\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "        except (SSLError, MaxRetryError):\n",
    "            print(f\"Failed request to url: {url}.\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    result = r.html.find(selector)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "async def scrape_author_page(s, url, item='name'):\n",
    "    if item == 'name':\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'id':\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'institution':\n",
    "        url_dep = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        selector = 'table.table tr td'\n",
    "        institution = await scrape_author_tab(s, url_dep, selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            result['department'] = institution[0].text\n",
    "            result['institution'] = institution[1].text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url_proj = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        projects = await scrape_author_tab(s, url_proj, selector)\n",
    "        project_list = []\n",
    "        for i in range(1,len(projects)):\n",
    "            project = projects[i].find('td a')[0].attrs['href']\n",
    "            project_list.append(project)\n",
    "        result = project_list\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url_group = url + '/orgs.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        groups = await scrape_author_tab(s, url_group, selector)\n",
    "        group_list = []\n",
    "        for i in range(1,len(groups)):\n",
    "            group = groups[i].find('td a')[0].attrs['href']\n",
    "            group_list.append(group)\n",
    "        result = group_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "async def scrape_author(s, url):\n",
    "    result = await asyncio.gather(\n",
    "            scrape_author_page(s, url, 'name'),\n",
    "            scrape_author_page(s, url, 'id'),\n",
    "            scrape_author_page(s, url, 'institution'),\n",
    "            scrape_author_page(s, url, 'projects'),\n",
    "            scrape_author_page(s, url, 'groups')\n",
    "        )\n",
    "    \n",
    "    author = {}\n",
    "    author['name'] = result[0]\n",
    "    author['id'] = result[1]\n",
    "    try:\n",
    "        author['department'] = result[2]['department']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        author['institution'] = result[2]['institution']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    author['projects'] = result[3]\n",
    "    author['groups'] = result[4]\n",
    "    \n",
    "    return author\n",
    "        \n",
    "    \n",
    "async def scrape_authors(urls):\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (scrape_author(s, url) for url in urls)\n",
    "    \n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35837a-ead6-4b36-94b9-9a8efc968b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_authors_batch(urls, start_pos=0, batch_size=100, out_file=None):\n",
    "    \"\"\"Scrape author pages in batches.\"\"\"\n",
    "    \n",
    "    batch_urls = [urls[i:i+batch_size] for i in range(0, len(urls), batch_size)]\n",
    "    \n",
    "    print(f\"Scraping {len(urls)-start_pos} author pages in {len(batch_urls)} batches of {batch_size}.\")\n",
    "    if out_file:\n",
    "        print(f\"Saving results to {out_file}.\")\n",
    "    \n",
    "    if out_file:\n",
    "        result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "        \n",
    "    result = []\n",
    "    for i, batch in enumerate(batch_urls):\n",
    "        print(f\"Scraping batch: {i+1}/{len(batch_urls)}. Authors: {i*batch_size}-{(i+1)*batch_size-1}.\", end=\"\\r\")\n",
    "        \n",
    "        t1 = time.perf_counter()\n",
    "        author_result = await scrape_authors(batch)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        # Print estimated time left\n",
    "        seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "        m, s = divmod(seconds_left, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        \n",
    "        print(f\"Last batch: {t2-t1:.2f} seconds. Estimated time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\" \")\n",
    "        \n",
    "        result.extend(author_result)\n",
    "        \n",
    "        if out_file:\n",
    "            result_df = result_df.append(author_result, ignore_index=True)\n",
    "            result_df.to_csv(out_file, index=None)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Nodelist: Scrape authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "## Get links to author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b020e-9e1c-4d37-918c-52b628ad7037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get links to author pages (takes 1m30s)\n",
    "# author_urls = await scrape('author_links')\n",
    "\n",
    "## Save author URLs\n",
    "\n",
    "# author_urls_df = pd.DataFrame(author_urls, columns=['author_urls'])\n",
    "# author_urls_df.to_csv('./data/author_urls.csv', index=False)\n",
    "\n",
    "## Read author URLs\n",
    "\n",
    "author_urls = pd.read_csv('./data/author_urls.csv')\n",
    "author_urls = list(author_urls['author_urls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "## Scrape author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692683a7-b73f-4829-8ccd-dececd1b7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build urls\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in author_urls]\n",
    "\n",
    "# Run in batch\n",
    "batch_size=200\n",
    "out_file = './data/nodelist_batch.csv'\n",
    "\n",
    "author_data = await scrape_authors_batch(urls, start_pos=0, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5c9f0-47a2-443f-a947-c4aa87218e35",
   "metadata": {},
   "source": [
    "# EXTRA CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c403a7-a638-46b2-8bdd-abe3a0682f02",
   "metadata": {},
   "source": [
    "## Speed Test: Sync vs Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c79c9-ca6c-4920-a254-78aefaf90948",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=crisrp&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Researchers&sort_by=crisrp.fullName_sort&order=asc&rpp=300&etal=0&start='\n",
    "\n",
    "urls = [url_root + str(page*100) for page in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c23e1-d21d-49ce-977c-43650099bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "def get_author_links_sync(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "    \n",
    "def main_sync(urls):\n",
    "    s = HTMLSession()\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        rows = get_author_links_sync(s, url)\n",
    "        result.append(rows)\n",
    "    return rows\n",
    "        \n",
    "async def get_author_links(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = await s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "\n",
    "async def main(urls):\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (get_author_links(s, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "        \n",
    "t1 = time.perf_counter()\n",
    "result = await main(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Async: {t2-t1:.2f} seconds.)\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "result_sync = main_sync(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Sync: {t2-t1:.2f} seconds.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
