{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b193e-f76d-4d81-ae23-9fac454cc603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    \"\"\"\n",
    "    Returns True if run in interactive mode (Jupyter, IPython or Python terminal prompt)\n",
    "    Returns False if run in script.\n",
    "    \"\"\"\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_box = r.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383baca-cdef-44ff-8ff7-dcb896b0f0d5",
   "metadata": {},
   "source": [
    "## Helper functions for individual and groups of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b3505-7daf-4487-8c26-63ed50296e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='author_links', attempts=10):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "    \"\"\"\n",
    "    # print(f\"Scraping url: {url}\")\n",
    "    \n",
    "    if items == 'authors':\n",
    "        result = await asyncio.gather(\n",
    "                scrape_author_page(s, url, 'name'),\n",
    "                scrape_author_page(s, url, 'id'),\n",
    "                scrape_author_page(s, url, 'institution'),\n",
    "                scrape_author_page(s, url, 'projects'),\n",
    "                scrape_author_page(s, url, 'groups')\n",
    "            )\n",
    "\n",
    "        author = {}\n",
    "        author['name'] = result[0]\n",
    "        author['id'] = result[1]\n",
    "        try:\n",
    "            author['department'] = result[2]['department']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            author['institution'] = result[2]['institution']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        author['projects'] = result[3]\n",
    "        author['groups'] = result[4]\n",
    "\n",
    "        return author\n",
    "        \n",
    "    elif items == 'papers':\n",
    "        not_found_msg = 'No ha estat possible trobar el que esteu buscant'\n",
    "        \n",
    "        for _ in range(attempts):\n",
    "            try:\n",
    "                r = await s.get(url)\n",
    "                title = r.html.find('title', first=True)\n",
    "                table = r.html.find('table.table', first=True)\n",
    "                rows = table.find('tr')\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "        else: # Failed all attempts\n",
    "            paper = {}\n",
    "            paper['url']         = url\n",
    "            paper['url_id']      = url[31:].split('?')[0]\n",
    "            paper['status code'] = r.status_code\n",
    "            \n",
    "            try:\n",
    "                if not_found_msg in title.text:\n",
    "                    paper['status description'] = 'Title: not found'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return paper\n",
    "\n",
    "        paper = {}\n",
    "        paper['url']    = url\n",
    "        paper['url_id'] = url[31:].split('?')[0]\n",
    "        \n",
    "        for row in rows:\n",
    "            # Get columns in row\n",
    "            columns = row.find('td')\n",
    "            \n",
    "            # Skip if empty row\n",
    "            if len(columns) == 0:\n",
    "                continue\n",
    "            \n",
    "            attributes = {\n",
    "                'dc.contributor.authors' : 'authors', \n",
    "                'dc.date.issued'         : 'date',\n",
    "                'dc.publisher'           : 'publisher',\n",
    "                'dc.identifier.citation' : 'citation',\n",
    "                'dc.identifier.issn'     : 'issn',\n",
    "                'dc.identifier.uri'      : 'uri',\n",
    "                'dc.identifier.isbn'    : 'isbn',\n",
    "                'dc.relation.ispartof'   : 'published_in',\n",
    "                'dc.title'               : 'title',\n",
    "                'dc.type'                : 'type',\n",
    "                'dc.identifier.doi'      : 'doi', \n",
    "                'dc.identifier.sourceid' : 'sourceid',\n",
    "                'dc.identifier.sourceref': 'sourceref',\n",
    "                'Appears in Collections:': 'appears_in_collections'}\n",
    "\n",
    "            for row_index in attributes.keys():\n",
    "                if columns[0].text == row_index:\n",
    "                    paper[attributes[row_index]] = columns[1].text\n",
    "\n",
    "        # Get authors ORCid'ss\n",
    "        simple_url = url.split('?')[0] + '?mode=simple'\n",
    "        for _ in range(attempts):\n",
    "            try:\n",
    "                r = await s.get(simple_url)\n",
    "                title = r.html.find('title', first=True)\n",
    "                table = r.html.find('table.table', first=True)\n",
    "                rows = table.find('tr')\n",
    "                authors = r.html.find('table.table a.authority.author')\n",
    "                author_hrefs = []\n",
    "                for author in authors:\n",
    "                    href = author.attrs['href']\n",
    "                    if href[:7] == '/orcid/':\n",
    "                        href = href[7:]\n",
    "                    author_hrefs.append(href)\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "        else: # Failed all attempts\n",
    "            paper = {}\n",
    "            paper['url']         = url\n",
    "            paper['url_id']      = url[31:].split('?')[0]\n",
    "            paper['status code'] = r.status_code\n",
    "            \n",
    "            try:\n",
    "                if not_found_msg in title.text:\n",
    "                    paper['status description'] = 'Title: not found'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return paper\n",
    "        \n",
    "        paper['orcids'] = author_hrefs\n",
    "        \n",
    "        return paper\n",
    "            \n",
    "        \n",
    "    else: # paper_links or author_links\n",
    "        while True:\n",
    "            try:\n",
    "                r = await s.get(url)\n",
    "                table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "                rows = table.find('tr')\n",
    "            except AttributeError:\n",
    "                with open('errors.txt', 'a') as f:\n",
    "                    f.write(f\"Could not find row in url: {url}\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        result = []\n",
    "            \n",
    "        for row in rows:\n",
    "            # Get columns in row\n",
    "            columns = row.find('td')\n",
    "            \n",
    "            # Skip if empty row\n",
    "            if len(columns) == 0:\n",
    "                continue\n",
    "\n",
    "            if items == 'paper_links':\n",
    "                # Get paper link\n",
    "                paper_link = columns[1].find('a')[0].attrs['href']\n",
    "                scrape_item = paper_link\n",
    "\n",
    "            if items == 'author_links':\n",
    "                # Get paper link\n",
    "                author_link = columns[0].find('a')[0].attrs['href']\n",
    "                scrape_item = author_link\n",
    "                \n",
    "            # Append to results_list\n",
    "            result.append(scrape_item)\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "async def scrape_urls(s, urls, items='author_links'):\n",
    "    \"\"\"Wrapper to scrape a list of urls.\"\"\"\n",
    "    tasks = (scrape_url(s, url, items) for url in urls)\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd0481-5196-403b-b52a-960310684044",
   "metadata": {},
   "source": [
    "## Helper functions for author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9d361-6e80-46ce-a5e7-46640d35a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author_tab(s, url, selector):\n",
    "    while True:\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "        except (SSLError, MaxRetryError):\n",
    "            print(f\"Failed request to url: {url}.\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    result = r.html.find(selector)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "async def scrape_author_page(s, url, item='name'):\n",
    "    if item == 'name':\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'id':\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'institution':\n",
    "        url_dep = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        selector = 'table.table tr td'\n",
    "        institution = await scrape_author_tab(s, url_dep, selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            result['department'] = institution[0].text\n",
    "            result['institution'] = institution[1].text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url_proj = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        projects = await scrape_author_tab(s, url_proj, selector)\n",
    "        project_list = []\n",
    "        for i in range(1,len(projects)):\n",
    "            project = projects[i].find('td a')[0].attrs['href']\n",
    "            project_list.append(project)\n",
    "        result = project_list\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url_group = url + '/orgs.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        groups = await scrape_author_tab(s, url_group, selector)\n",
    "        group_list = []\n",
    "        for i in range(1,len(groups)):\n",
    "            group = groups[i].find('td a')[0].attrs['href']\n",
    "            group_list.append(group)\n",
    "        result = group_list\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a07b54-6be2-43f7-81fb-4186b46acc55",
   "metadata": {},
   "source": [
    "## Main Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91abdf87-81a7-4065-91d2-b36c9177bbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape(urls=None, items='authors', n_pages=None, start_pos=0, batch_size=None, out_file=None):\n",
    "    \"\"\"\n",
    "    Main entry function to scrape Portal de la Reserca.\n",
    "    Options:\n",
    "        urls: list of urls. If items='paper_links' this is not needed.\n",
    "        items: [authors, papers, author_links, paper_links]\n",
    "        start_pos: starting position\n",
    "        batch_size: batch size\n",
    "        out_file: output file \n",
    "    \"\"\"\n",
    "    \n",
    "    if items == 'author_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=crisrp'                              + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Researchers'               + \\\n",
    "            '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "        \n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "            \n",
    "            \n",
    "    elif items == 'paper_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=publications'                        + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Items'                     + \\\n",
    "            '&filter_field_2=itemtype'                      + \\\n",
    "                '&filter_type_2=notequals'                  + \\\n",
    "                '&filter_value_2=Phd+Thesis'                + \\\n",
    "            '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "\n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "        \n",
    "            if not urls:\n",
    "                urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "        \n",
    "    if not batch_size:\n",
    "        print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "\n",
    "        urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "\n",
    "        s = AsyncHTMLSession()\n",
    "\n",
    "        print(f\"Scraping {len(urls)} URLs...\")\n",
    "        t1 = time.perf_counter()\n",
    "        result = await scrape_urls(s, urls, items=items)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # Gather all results into single list\n",
    "        result = [href for sublist in result for href in sublist]\n",
    "\n",
    "        print(f\"Scraped {len(result)} items in {t2-t1:.2f} seconds.\")\n",
    "        \n",
    "        if out_file:\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df.to_csv(out_file, index=None)\n",
    "            print(f\"Saved results to {out_file}.\")\n",
    "\n",
    "   \n",
    "    if batch_size: \n",
    "\n",
    "        if not urls:\n",
    "            raise TypeError(\"Must provide list of urls or set items='paper_links'\")\n",
    "\n",
    "        batch_urls = [urls[i:i+batch_size] for i in range(start_pos, len(urls), batch_size)]\n",
    "\n",
    "        print(f\"Scraping {len(urls)-start_pos} {items} in {len(batch_urls)} batches of {batch_size}, starting in position {start_pos}.\")\n",
    "        \n",
    "        if out_file:\n",
    "            print(f\"Saving results to {out_file}.\")\n",
    "            if items == 'authors':\n",
    "                result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "            elif items == 'paper_links':\n",
    "                result_df = pd.DataFrame()\n",
    "            elif items == 'papers':\n",
    "                result_df = pd.DataFrame()\n",
    "\n",
    "        result = []\n",
    "        for i, batch in enumerate(batch_urls):\n",
    "            print(f\"Scraping batch: {i+1}/{len(batch_urls)}. {items}: {i*batch_size}-{(i+1)*batch_size-1}.\", end=\"\\r\")\n",
    "            s = AsyncHTMLSession()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            batch_result = await scrape_urls(s, batch, items=items)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            if items == 'paper_links':\n",
    "                # Flatten result\n",
    "                batch_result = [i for sublist in batch_result for i in sublist]\n",
    "\n",
    "            # Print estimated time left\n",
    "            seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "            m, s = divmod(seconds_left, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "\n",
    "            print(f\"Last batch: {t2-t1:.2f} seconds. Estimated time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\" \")\n",
    "\n",
    "            result.extend(batch_result)\n",
    "\n",
    "            if out_file:\n",
    "                result_df = result_df.append(batch_result, ignore_index=True)\n",
    "                result_df.to_csv(out_file, index=None)\n",
    "\n",
    "        print(\"\\nDone.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb4382-8dbc-4b12-93d5-e4700342dec4",
   "metadata": {},
   "source": [
    "# Run Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nodelist: Scrape authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "### Get links to author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd7962-2e6b-4e9d-ac1c-fa48a538e0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get links to author pages (takes 1m30s)\n",
    "items = 'author_links'\n",
    "out_file = './data/author_urls_test.csv'\n",
    "author_urls = await scrape(items=items, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "### Scrape author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e02e1f-7c70-4e46-9928-c1121591c2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build urls\n",
    "author_urls = pd.read_csv('./data/author_urls.csv')\n",
    "author_urls = list(author_urls['author_urls'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in author_urls]\n",
    "\n",
    "# Get author data in batch\n",
    "items = 'authors'\n",
    "batch_size = 20\n",
    "out_file = './data/nodelist_test.csv'\n",
    "\n",
    "author_data = await scrape(urls=urls, items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd803dc5-ca2b-4d2b-8db4-63fa48867dac",
   "metadata": {},
   "source": [
    "## Edgelist: scrape publications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e846e-310f-4b0e-afbc-7c32e0162817",
   "metadata": {},
   "source": [
    "### Get links to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed63696-3202-4545-ba52-6bb207a2a122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get links to papers in batch\n",
    "items = 'paper_links'\n",
    "batch_size = 20\n",
    "out_file = './data/paper_links_test.csv'\n",
    "\n",
    "paper_links = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376263f1-2ef7-4157-b82f-e0e7bfa89087",
   "metadata": {},
   "source": [
    "### Get coauthors in paper pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32ace0-3e1a-4427-977e-9778eb34f367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build urls\n",
    "paper_urls = pd.read_csv('./data/paper_links.csv')\n",
    "paper_urls = list(paper_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url + '?mode=full' for url in paper_urls]\n",
    "\n",
    "# Run in batch\n",
    "items = 'papers'\n",
    "batch_size = 200\n",
    "# start_pos = 56040 # old starting position with problems\n",
    "# start_pos = 0\n",
    "out_file = './data/papers.csv'\n",
    "\n",
    "if is_interactive():\n",
    "    papers = await scrape(urls=urls, items=items, batch_size=batch_size, out_file=out_file)\n",
    "else:\n",
    "    papers = asyncio.run(scrape(urls=urls, items=items, batch_size=batch_size, out_file=out_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30d3f4-f43d-42af-962a-d9904e273d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
