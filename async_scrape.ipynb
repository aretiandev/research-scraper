{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1b193e-f76d-4d81-ae23-9fac454cc603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_box = r.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1383baca-cdef-44ff-8ff7-dcb896b0f0d5",
   "metadata": {},
   "source": [
    "## Helper functions for individual and groups of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1b3505-7daf-4487-8c26-63ed50296e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "    \"\"\"\n",
    "    # print(f\"Scraping url: {url}\")\n",
    "    \n",
    "    if items == 'authors':\n",
    "        result = await asyncio.gather(\n",
    "                scrape_author_page(s, url, 'name'),\n",
    "                scrape_author_page(s, url, 'id'),\n",
    "                scrape_author_page(s, url, 'institution'),\n",
    "                scrape_author_page(s, url, 'projects'),\n",
    "                scrape_author_page(s, url, 'groups')\n",
    "            )\n",
    "\n",
    "        author = {}\n",
    "        author['name'] = result[0]\n",
    "        author['id'] = result[1]\n",
    "        try:\n",
    "            author['department'] = result[2]['department']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            author['institution'] = result[2]['institution']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        author['projects'] = result[3]\n",
    "        author['groups'] = result[4]\n",
    "\n",
    "        return author\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                r = await s.get(url)\n",
    "                # table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "                table = r.html.find('table.table', first=True)\n",
    "                rows = table.find('tr')\n",
    "            except AttributeError:\n",
    "                with open('errors.txt', 'a') as f:\n",
    "                    f.write(f\"Could not find row in url: {url}\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        if items == 'papers':\n",
    "            result = {}\n",
    "        else:\n",
    "            result = []\n",
    "            \n",
    "        for row in rows:\n",
    "            # Get columns in row\n",
    "            columns = row.find('td')\n",
    "            \n",
    "            # Skip if empty row\n",
    "            if len(columns) == 0:\n",
    "                continue\n",
    "\n",
    "            if items == 'paper_links':\n",
    "                # Get paper link\n",
    "                paper_link = columns[1].find('a')[0].attrs['href']\n",
    "                scrape_item = paper_link\n",
    "                # Append to results_list\n",
    "                result.append(scrape_item)\n",
    "\n",
    "            if items == 'author_links':\n",
    "                # Get paper link\n",
    "                author_link = columns[0].find('a')[0].attrs['href']\n",
    "                scrape_item = author_link\n",
    "                # Append to results_list\n",
    "                result.append(scrape_item)\n",
    "            \n",
    "            if items == 'papers':\n",
    "                \n",
    "                attributes = {\n",
    "                    'dc.contributor.authors' : 'authors', \n",
    "                    'dc.date.issued'         : 'date',\n",
    "                    'dc.publisher'           : 'publisher',\n",
    "                    'dc.identifier.citation' : 'citation',\n",
    "                    'dc.identifier.issn'     : 'issn',\n",
    "                    'dc.identifier.uri'      : 'uri',\n",
    "                    'dc.identifier.isbn'    : 'isbn',\n",
    "                    'dc.relation.ispartof'   : 'published_in',\n",
    "                    'dc.title'               : 'title',\n",
    "                    'dc.type'                : 'type',\n",
    "                    'dc.identifier.doi'      : 'doi', \n",
    "                    'dc.identifier.sourceid' : 'sourceid',\n",
    "                    'dc.identifier.sourceref': 'sourceref',\n",
    "                    'Appears in Collections:': 'appears_in_collections'}\n",
    "                 \n",
    "                for row_index in attributes.keys():\n",
    "                    if columns[0].text == row_index:\n",
    "                        result[attributes[row_index]] = columns[1].text\n",
    "                \n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "async def scrape_urls(s, urls, items='author_links'):\n",
    "    \"\"\"Wrapper to scrape a list of urls.\"\"\"\n",
    "    tasks = (scrape_url(s, url, items) for url in urls)\n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd0481-5196-403b-b52a-960310684044",
   "metadata": {},
   "source": [
    "## Helper functions for author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c9d361-6e80-46ce-a5e7-46640d35a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author_tab(s, url, selector):\n",
    "    while True:\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "        except (SSLError, MaxRetryError):\n",
    "            print(f\"Failed request to url: {url}.\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    result = r.html.find(selector)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "async def scrape_author_page(s, url, item='name'):\n",
    "    if item == 'name':\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'id':\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'institution':\n",
    "        url_dep = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        selector = 'table.table tr td'\n",
    "        institution = await scrape_author_tab(s, url_dep, selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            result['department'] = institution[0].text\n",
    "            result['institution'] = institution[1].text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url_proj = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        projects = await scrape_author_tab(s, url_proj, selector)\n",
    "        project_list = []\n",
    "        for i in range(1,len(projects)):\n",
    "            project = projects[i].find('td a')[0].attrs['href']\n",
    "            project_list.append(project)\n",
    "        result = project_list\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url_group = url + '/orgs.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        groups = await scrape_author_tab(s, url_group, selector)\n",
    "        group_list = []\n",
    "        for i in range(1,len(groups)):\n",
    "            group = groups[i].find('td a')[0].attrs['href']\n",
    "            group_list.append(group)\n",
    "        result = group_list\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a07b54-6be2-43f7-81fb-4186b46acc55",
   "metadata": {},
   "source": [
    "## Main Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91abdf87-81a7-4065-91d2-b36c9177bbba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape(urls=None, items='authors', n_pages=None, start_pos=0, batch_size=None, out_file=None):\n",
    "    \"\"\"\n",
    "    Main entry function to scrape Portal de la Reserca.\n",
    "    Options:\n",
    "        urls: list of urls. If items='paper_links' this is not needed.\n",
    "        items: [authors, papers, author_links, paper_links]\n",
    "        start_pos: starting position\n",
    "        batch_size: batch size\n",
    "        out_file: output file \n",
    "    \"\"\"\n",
    "    \n",
    "    if items == 'author_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=crisrp'                              + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Researchers'               + \\\n",
    "            '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "        \n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "            \n",
    "            \n",
    "    elif items == 'paper_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=publications'                        + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Items'                     + \\\n",
    "            '&filter_field_2=itemtype'                      + \\\n",
    "                '&filter_type_2=notequals'                  + \\\n",
    "                '&filter_value_2=Phd+Thesis'                + \\\n",
    "            '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "\n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "        \n",
    "            if not urls:\n",
    "                urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "        \n",
    "    if not batch_size:\n",
    "        print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "\n",
    "        urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "\n",
    "        s = AsyncHTMLSession()\n",
    "\n",
    "        print(f\"Scraping {len(urls)} URLs...\")\n",
    "        t1 = time.perf_counter()\n",
    "        result = await scrape_urls(s, urls, items=items)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # Gather all results into single list\n",
    "        result = [href for sublist in result for href in sublist]\n",
    "\n",
    "        print(f\"Scraped {len(result)} items in {t2-t1:.2f} seconds.\")\n",
    "        \n",
    "        if out_file:\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df.to_csv(out_file, index=None)\n",
    "            print(f\"Saved results to {out_file}.\")\n",
    "\n",
    "   \n",
    "    if batch_size: \n",
    "\n",
    "        if not urls:\n",
    "            raise TypeError(\"Must provide list of urls or set items='paper_links'\")\n",
    "\n",
    "        batch_urls = [urls[i:i+batch_size] for i in range(0, len(urls), batch_size)]\n",
    "\n",
    "        print(f\"Scraping {len(urls)-start_pos} {items} in {len(batch_urls)} batches of {batch_size}.\")\n",
    "        if out_file:\n",
    "            print(f\"Saving results to {out_file}.\")\n",
    "            if items == 'authors':\n",
    "                result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "            elif items == 'paper_links':\n",
    "                result_df = pd.DataFrame()\n",
    "            elif items == 'papers':\n",
    "                result_df = pd.DataFrame()\n",
    "\n",
    "        result = []\n",
    "        for i, batch in enumerate(batch_urls):\n",
    "            print(f\"Scraping batch: {i+1}/{len(batch_urls)}. {items}: {i*batch_size}-{(i+1)*batch_size-1}.\", end=\"\\r\")\n",
    "            s = AsyncHTMLSession()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            batch_result = await scrape_urls(s, batch, items=items)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            if items == 'paper_links':\n",
    "                # Flatten result\n",
    "                batch_result = [i for sublist in batch_result for i in sublist]\n",
    "\n",
    "            # Print estimated time left\n",
    "            seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "            m, s = divmod(seconds_left, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "\n",
    "            print(f\"Last batch: {t2-t1:.2f} seconds. Estimated time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\" \")\n",
    "\n",
    "            result.extend(batch_result)\n",
    "\n",
    "            if out_file:\n",
    "                result_df = result_df.append(batch_result, ignore_index=True)\n",
    "                result_df.to_csv(out_file, index=None)\n",
    "\n",
    "        print(\"\\nDone.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb4382-8dbc-4b12-93d5-e4700342dec4",
   "metadata": {},
   "source": [
    "# Run Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nodelist: Scrape authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "### Get links to author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfd7962-2e6b-4e9d-ac1c-fa48a538e0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of pages to scrape.\n",
      "Scraping author_links from Portal de la Reserca.\n",
      "Scraping 64 URLs...\n",
      "Scraped 19050 items in 82.83 seconds.\n",
      "Saved results to ./data/author_urls_test.csv.\n"
     ]
    }
   ],
   "source": [
    "# Get links to author pages (takes 1m30s)\n",
    "items = 'author_links'\n",
    "out_file = './data/author_urls_test.csv'\n",
    "author_urls = await scrape(items=items, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "### Scrape author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e02e1f-7c70-4e46-9928-c1121591c2db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 19050 authors in 953 batches of 20.\n",
      "Saving results to ./data/nodelist_test.csv.\n",
      "Last batch: 8.06 seconds. Estimated time left: 2h7m52s. Scraping batch: 3/953. authors: 40-59..\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SSLError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_tab\u001b[0;34m(s, url, selector)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/770962718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/nodelist_test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mauthor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(urls, items, n_pages, start_pos, batch_size, out_file)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_urls\u001b[0;34m(s, urls, items)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Wrapper to scrape a list of urls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(s, url, items)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'authors'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         result = await asyncio.gather(\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mscrape_author_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mscrape_author_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_page\u001b[0;34m(s, url, item)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0murl_dep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/researcherdepartaments.html?onlytab=true'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'table.table tr td'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0minstitution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_author_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_dep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_tab\u001b[0;34m(s, url, selector)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed request to url: {url}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SSLError' is not defined"
     ]
    }
   ],
   "source": [
    "# Build urls\n",
    "author_urls = pd.read_csv('./data/author_urls.csv')\n",
    "author_urls = list(author_urls['author_urls'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in author_urls]\n",
    "\n",
    "# Get author data in batch\n",
    "items = 'authors'\n",
    "batch_size = 20\n",
    "out_file = './data/nodelist_test.csv'\n",
    "\n",
    "author_data = await scrape(urls=urls, items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd803dc5-ca2b-4d2b-8db4-63fa48867dac",
   "metadata": {},
   "source": [
    "## Edgelist: scrape publications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e846e-310f-4b0e-afbc-7c32e0162817",
   "metadata": {},
   "source": [
    "### Get links to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed63696-3202-4545-ba52-6bb207a2a122",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of pages to scrape.\n",
      "Scraping 2240 paper_links in 112 batches of 20.\n",
      "Saving results to ./data/paper_links_test.csv.\n",
      "Last batch: 18.65 seconds. Estimated time left: 0h34m30s. Scraping batch: 3/112. paper_links: 40-59.\r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(s, url, items)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# table = r.html.find('div.panel.panel-info table.table', first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/704596701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/paper_links_test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpaper_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(urls, items, n_pages, start_pos, batch_size, out_file)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_urls\u001b[0;34m(s, urls, items)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Wrapper to scrape a list of urls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get links to papers in batch\n",
    "items = 'paper_links'\n",
    "batch_size = 20\n",
    "out_file = './data/paper_links_test.csv'\n",
    "\n",
    "paper_links = await scrape(items=items, batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376263f1-2ef7-4157-b82f-e0e7bfa89087",
   "metadata": {},
   "source": [
    "### Get coauthors in paper pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b0d4b4-48c2-4f62-8733-d922a9b8cd87",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 671822 papers in 3360 batches of 200.\n",
      "Saving results to ./data/coauthors.csv.\n",
      "Last batch: 24.03 seconds. Estimated time left: 22h25m24s. Scraping batch: 2/3360. papers: 200-399.\r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(s, url, items)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# table = r.html.find('div.panel.panel-info table.table', first=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6925/313049752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/coauthors.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(urls, items, n_pages, start_pos, batch_size, out_file)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6925/3341413848.py\u001b[0m in \u001b[0;36mscrape_urls\u001b[0;34m(s, urls, items)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"\"\"Wrapper to scrape a list of urls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build urls\n",
    "paper_urls = pd.read_csv('./data/paper_links.csv')\n",
    "paper_urls = list(paper_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url + '?mode=full' for url in paper_urls]\n",
    "\n",
    "# Run in batch\n",
    "items = 'papers'\n",
    "batch_size = 200\n",
    "out_file = './data/coauthors.csv'\n",
    "\n",
    "papers = await scrape(urls=urls, items=items, batch_size=batch_size, out_file=out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
