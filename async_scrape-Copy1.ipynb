{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## Helper Functions for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0623840-a0bb-4fa0-9045-7a5177cccff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "    \"\"\"\n",
    "    # print(f\"Scraping url: {url}\")\n",
    "    \n",
    "    if items == 'authors':\n",
    "        result = await asyncio.gather(\n",
    "                scrape_author_page(s, url, 'name'),\n",
    "                scrape_author_page(s, url, 'id'),\n",
    "                scrape_author_page(s, url, 'institution'),\n",
    "                scrape_author_page(s, url, 'projects'),\n",
    "                scrape_author_page(s, url, 'groups')\n",
    "            )\n",
    "\n",
    "        author = {}\n",
    "        author['name'] = result[0]\n",
    "        author['id'] = result[1]\n",
    "        try:\n",
    "            author['department'] = result[2]['department']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        try:\n",
    "            author['institution'] = result[2]['institution']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        author['projects'] = result[3]\n",
    "        author['groups'] = result[4]\n",
    "\n",
    "        # scrape_item = author\n",
    "        return author\n",
    "        \n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "            table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "            rows = table.find('tr')\n",
    "        except AttributeError:\n",
    "            with open('errors.txt', 'a') as f:\n",
    "                f.write(f\"Could not find row in url: {url}\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for row in rows:\n",
    "        # Get columns in row\n",
    "        columns = row.find('td')\n",
    "        # Skip if empty row\n",
    "        if len(columns) == 0:\n",
    "            continue\n",
    "            \n",
    "        if items == 'paper_links':\n",
    "            # Get paper link\n",
    "            paper_link = columns[1].find('a')[0].attrs['href']\n",
    "            scrape_item = paper_link\n",
    "            \n",
    "        elif items == 'author_links':\n",
    "            # Get paper link\n",
    "            author_link = columns[0].find('a')[0].attrs['href']\n",
    "            scrape_item = author_link\n",
    "            \n",
    "        elif items == 'papers':\n",
    "            # Get paper data\n",
    "            paper_date    = columns[0].text\n",
    "            paper_title   = columns[1].text\n",
    "            paper_authors = columns[2].text\n",
    "            paper_type    = columns[3].text\n",
    "\n",
    "            paper = {}\n",
    "            paper['date'] = paper_date\n",
    "            paper['title'] = paper_title\n",
    "            paper['type'] = paper_type\n",
    "\n",
    "            paper_authors_list= paper_authors.split(';')\n",
    "            for i, author in enumerate(paper_authors_list):\n",
    "                paper[f\"author_{i}\"] = author\n",
    "\n",
    "            scrape_item = paper\n",
    "\n",
    "        # Append to paper links list\n",
    "        result_list.append(scrape_item)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "\n",
    "# async def scrape_urls(s, urls, items='author_links'):\n",
    "#     \"\"\"Wrapper of scrape_url to scrape a list of urls.\"\"\"\n",
    "#     tasks = (scrape_url(s, url, items) for url in urls)\n",
    "#     return await asyncio.gather(*tasks)\n",
    "\n",
    "async def scrape_urls(s, urls, items='author_links'):\n",
    "    \"\"\"Wrapper to scrape a list of urls.\"\"\"\n",
    "    tasks = (scrape_url(s, url, items) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_box = r.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)\n",
    "\n",
    "\n",
    "# async def scrape(items='author_links', n_pages=None):\n",
    "#     \"\"\"\n",
    "#     Scrape Portal de la Reserca.\n",
    "#     Options:\n",
    "#         items   = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "#         n_pages = Number of pages to scrape.\n",
    "#     \"\"\"\n",
    "#     print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "    \n",
    "#     if items == 'author_links':\n",
    "#         url_root =                                            \\\n",
    "#             'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "#             'query='                                        + \\\n",
    "#             '&location=crisrp'                              + \\\n",
    "#             '&filter_field_1=resourcetype'                  + \\\n",
    "#                 '&filter_type_1=equals'                     + \\\n",
    "#                 '&filter_value_1=Researchers'               + \\\n",
    "#             '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "#                 '&order=asc'                                + \\\n",
    "#             '&rpp=300'                                      + \\\n",
    "#             '&etal=0'                                       + \\\n",
    "#             '&start='\n",
    "        \n",
    "#     elif items == 'paper_links':\n",
    "#         url_root =                                            \\\n",
    "#             'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "#             'query='                                        + \\\n",
    "#             '&location=publications'                        + \\\n",
    "#             '&filter_field_1=resourcetype'                  + \\\n",
    "#                 '&filter_type_1=equals'                     + \\\n",
    "#                 '&filter_value_1=Items'                     + \\\n",
    "#             '&filter_field_2=itemtype'                      + \\\n",
    "#                 '&filter_type_2=notequals'                  + \\\n",
    "#                 '&filter_value_2=Phd+Thesis'                + \\\n",
    "#             '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "#                 '&order=asc'                                + \\\n",
    "#             '&rpp=300'                                      + \\\n",
    "#             '&etal=0'                                       + \\\n",
    "#             '&start='\n",
    "        \n",
    "#     if not n_pages:\n",
    "#         print(\"Calculating number of pages to scrape.\")\n",
    "#         max_pages = get_max_pages(url_root + '0')\n",
    "#         n_pages = max_pages\n",
    "\n",
    "#     urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "\n",
    "#     s = AsyncHTMLSession()\n",
    "    \n",
    "#     print(f\"Scraping {len(urls)} URLs...\")\n",
    "#     t1 = time.perf_counter()\n",
    "#     result = await scrape_urls(s, urls, items=items)\n",
    "#     t2 = time.perf_counter()\n",
    "    \n",
    "#     # Gather all results into single list\n",
    "#     full_list = [href for sublist in result for href in sublist]\n",
    "    \n",
    "#     print(f\"Scraped {len(full_list)} items in {t2-t1:.2f} seconds.\")\n",
    "    \n",
    "#     return full_list\n",
    "   \n",
    "    \n",
    "async def scrape(urls=None, items='authors', n_pages=None, start_pos=0, batch_size=None, out_file=None):\n",
    "    \"\"\"\n",
    "    Scrape Portal de la Reserca in batches:.\n",
    "    Options:\n",
    "        urls: list of urls. If items='paper_links' this is not needed.\n",
    "        items: [authors, papers, author_links, paper_links]\n",
    "        start_pos: starting position\n",
    "        batch_size: batch size\n",
    "        out_file: output file \n",
    "    \"\"\"\n",
    "    \n",
    "    if items == 'author_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=crisrp'                              + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Researchers'               + \\\n",
    "            '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "        \n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "            \n",
    "            \n",
    "    elif items == 'paper_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=publications'                        + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Items'                     + \\\n",
    "            '&filter_field_2=itemtype'                      + \\\n",
    "                '&filter_type_2=notequals'                  + \\\n",
    "                '&filter_value_2=Phd+Thesis'                + \\\n",
    "            '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "\n",
    "        if not n_pages:\n",
    "            print(\"Calculating number of pages to scrape.\")\n",
    "            max_pages = get_max_pages(url_root + '0')\n",
    "            n_pages = max_pages\n",
    "        \n",
    "            if not urls:\n",
    "                urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "        \n",
    "    if not batch_size:\n",
    "        print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "\n",
    "        urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "\n",
    "        s = AsyncHTMLSession()\n",
    "\n",
    "        print(f\"Scraping {len(urls)} URLs...\")\n",
    "        t1 = time.perf_counter()\n",
    "        result = await scrape_urls(s, urls, items=items)\n",
    "        t2 = time.perf_counter()\n",
    "\n",
    "        # Gather all results into single list\n",
    "        result = [href for sublist in result for href in sublist]\n",
    "\n",
    "        print(f\"Scraped {len(result)} items in {t2-t1:.2f} seconds.\")\n",
    "        \n",
    "        if out_file:\n",
    "            result_df = pd.DataFrame(result)\n",
    "            result_df.to_csv(out_file, index=None)\n",
    "            print(f\"Saved results to {out_file}.\")\n",
    "\n",
    "   \n",
    "    if batch_size: \n",
    "\n",
    "        if not urls:\n",
    "            raise TypeError(\"Must provide list of urls or set items='paper_links'\")\n",
    "\n",
    "        batch_urls = [urls[i:i+batch_size] for i in range(0, len(urls), batch_size)]\n",
    "\n",
    "        print(f\"Scraping {len(urls)-start_pos} {items} in {len(batch_urls)} batches of {batch_size}.\")\n",
    "        if out_file:\n",
    "            print(f\"Saving results to {out_file}.\")\n",
    "            if items == 'authors':\n",
    "                result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "            elif items == 'paper_links':\n",
    "                result_df = pd.DataFrame()\n",
    "\n",
    "        result = []\n",
    "        for i, batch in enumerate(batch_urls):\n",
    "            print(f\"Scraping batch: {i+1}/{len(batch_urls)}. {items}: {i*batch_size}-{(i+1)*batch_size-1}.\", end=\"\\r\")\n",
    "            s = AsyncHTMLSession()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            batch_result = await scrape_urls(s, batch, items=items)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            if items == 'paper_links':\n",
    "                # Flatten result\n",
    "                batch_result = [i for sublist in batch_result for i in sublist]\n",
    "\n",
    "            # Print estimated time left\n",
    "            seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "            m, s = divmod(seconds_left, 60)\n",
    "            h, m = divmod(m, 60)\n",
    "\n",
    "            print(f\"Last batch: {t2-t1:.2f} seconds. Estimated time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\" \")\n",
    "\n",
    "            result.extend(batch_result)\n",
    "\n",
    "            if out_file:\n",
    "                result_df = result_df.append(batch_result, ignore_index=True)\n",
    "                result_df.to_csv(out_file, index=None)\n",
    "\n",
    "        print(\"\\nDone.\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b8973-a368-4e42-b12f-a8efea8add6d",
   "metadata": {},
   "source": [
    "## Helper functions for Author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a78f6d-bc7e-4157-aed8-327dcba5cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author_tab(s, url, selector):\n",
    "    while True:\n",
    "        try:\n",
    "            r = await s.get(url)\n",
    "        except (SSLError, MaxRetryError):\n",
    "            print(f\"Failed request to url: {url}.\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        break\n",
    "    result = r.html.find(selector)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "async def scrape_author_page(s, url, item='name'):\n",
    "    if item == 'name':\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'id':\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        try:\n",
    "            result = result[0].text\n",
    "        except IndexError:\n",
    "            result = None\n",
    "    \n",
    "    elif item == 'institution':\n",
    "        url_dep = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        selector = 'table.table tr td'\n",
    "        institution = await scrape_author_tab(s, url_dep, selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            result['department'] = institution[0].text\n",
    "            result['institution'] = institution[1].text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url_proj = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        projects = await scrape_author_tab(s, url_proj, selector)\n",
    "        project_list = []\n",
    "        for i in range(1,len(projects)):\n",
    "            project = projects[i].find('td a')[0].attrs['href']\n",
    "            project_list.append(project)\n",
    "        result = project_list\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url_group = url + '/orgs.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        groups = await scrape_author_tab(s, url_group, selector)\n",
    "        group_list = []\n",
    "        for i in range(1,len(groups)):\n",
    "            group = groups[i].find('td a')[0].attrs['href']\n",
    "            group_list.append(group)\n",
    "        result = group_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# async def scrape_author(s, url):\n",
    "#     result = await asyncio.gather(\n",
    "#             scrape_author_page(s, url, 'name'),\n",
    "#             scrape_author_page(s, url, 'id'),\n",
    "#             scrape_author_page(s, url, 'institution'),\n",
    "#             scrape_author_page(s, url, 'projects'),\n",
    "#             scrape_author_page(s, url, 'groups')\n",
    "#         )\n",
    "    \n",
    "#     author = {}\n",
    "#     author['name'] = result[0]\n",
    "#     author['id'] = result[1]\n",
    "#     try:\n",
    "#         author['department'] = result[2]['department']\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "#     try:\n",
    "#         author['institution'] = result[2]['institution']\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "#     author['projects'] = result[3]\n",
    "#     author['groups'] = result[4]\n",
    "    \n",
    "#     return author\n",
    "        \n",
    "    \n",
    "# async def scrape_authors(s, urls):\n",
    "#     \"\"\"Wrapper for scrape_author to scrape a list of urls.\"\"\"\n",
    "#     tasks = (scrape_author(s, url) for url in urls)\n",
    "#     return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "# async def scrape_batch(urls=None, items='authors', start_pos=0, batch_size=100, out_file=None):\n",
    "#     \"\"\"\n",
    "#     Scrape Portal de la Reserca in batches:.\n",
    "#     Options:\n",
    "#         urls: list of urls. If items='paper_links' this is not needed.\n",
    "#         items: [authors, author_links, paper_links]\n",
    "#         start_pos: starting position\n",
    "#         batch_size: batch size\n",
    "#         out_file: output file \n",
    "#     \"\"\"\n",
    "    \n",
    "#     if items == 'paper_links':\n",
    "#         url_root =                                            \\\n",
    "#             'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "#             'query='                                        + \\\n",
    "#             '&location=publications'                        + \\\n",
    "#             '&filter_field_1=resourcetype'                  + \\\n",
    "#                 '&filter_type_1=equals'                     + \\\n",
    "#                 '&filter_value_1=Items'                     + \\\n",
    "#             '&filter_field_2=itemtype'                      + \\\n",
    "#                 '&filter_type_2=notequals'                  + \\\n",
    "#                 '&filter_value_2=Phd+Thesis'                + \\\n",
    "#             '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "#                 '&order=asc'                                + \\\n",
    "#             '&rpp=300'                                      + \\\n",
    "#             '&etal=0'                                       + \\\n",
    "#             '&start='\n",
    "        \n",
    "#         print(\"Calculating number of pages to scrape.\")\n",
    "#         # max_pages = get_max_pages(url_root + '0')\n",
    "#         max_pages = 2240 # <- hard code result\n",
    "#         n_pages = max_pages\n",
    "\n",
    "#         urls = [url_root + str(page*300) for page in range(n_pages)]\n",
    "    \n",
    "#     if not urls:\n",
    "#         raise TypeError(\"Must provide list of urls or set items='paper_links'\")\n",
    "    \n",
    "#     batch_urls = [urls[i:i+batch_size] for i in range(0, len(urls), batch_size)]\n",
    "    \n",
    "#     print(f\"Scraping {len(urls)-start_pos} {items} in {len(batch_urls)} batches of {batch_size}.\")\n",
    "#     if out_file:\n",
    "#         print(f\"Saving results to {out_file}.\")\n",
    "#         if items == 'authors':\n",
    "#             result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "#         elif items == 'paper_links':\n",
    "#             result_df = pd.DataFrame()\n",
    "        \n",
    "#     result = []\n",
    "#     for i, batch in enumerate(batch_urls):\n",
    "#         print(f\"Scraping batch: {i+1}/{len(batch_urls)}. {items}: {i*batch_size}-{(i+1)*batch_size-1}.\", end=\"\\r\")\n",
    "#         s = AsyncHTMLSession()\n",
    "        \n",
    "#         t1 = time.perf_counter()\n",
    "#         batch_result = await scrape_urls(s, batch, items=items)\n",
    "#         t2 = time.perf_counter()\n",
    "        \n",
    "#         if items == 'paper_links':\n",
    "#             # Flatten result\n",
    "#             batch_result = [i for sublist in batch_result for i in sublist]\n",
    "        \n",
    "#         # Print estimated time left\n",
    "#         seconds_left = (len(batch_urls)-i)*(t2-t1)\n",
    "#         m, s = divmod(seconds_left, 60)\n",
    "#         h, m = divmod(m, 60)\n",
    "        \n",
    "#         print(f\"Last batch: {t2-t1:.2f} seconds. Estimated time left: {h:.0f}h{m:.0f}m{s:.0f}s.\", end=\" \")\n",
    "        \n",
    "#         result.extend(batch_result)\n",
    "        \n",
    "#         if out_file:\n",
    "#             result_df = result_df.append(batch_result, ignore_index=True)\n",
    "#             result_df.to_csv(out_file, index=None)\n",
    "\n",
    "#     print(\"\\nDone.\")\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb4382-8dbc-4b12-93d5-e4700342dec4",
   "metadata": {},
   "source": [
    "# Run Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Nodelist: Scrape authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "### Get links to author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dfd7962-2e6b-4e9d-ac1c-fa48a538e0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of pages to scrape.\n",
      "Scraping author_links from Portal de la Reserca.\n",
      "Scraping 64 URLs...\n",
      "Scraped 19050 items in 100.13 seconds.\n",
      "Saved results to ./data/author_urls_test.csv.\n"
     ]
    }
   ],
   "source": [
    "# Get links to author pages (takes 1m30s)\n",
    "author_urls = await scrape(items='author_links', out_file='./data/author_urls_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "### Scrape author pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2145cdae-2e44-46c1-ad51-a1afe23da163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build urls\n",
    "author_urls = pd.read_csv('./data/author_urls.csv')\n",
    "author_urls = list(author_urls['author_urls'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in author_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "457e6916-5789-43d2-ac52-34ee1d39093c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 19050 authors in 953 batches of 20.\n",
      "Saving results to ./data/nodelist_test.csv.\n",
      "Last batch: 7.74 seconds. Estimated time left: 1h42m37s. Scraping batch: 158/953. authors: 3140-3159.Scraping batch: 105/953. authors: 2080-2099.Scraping batch: 112/953. authors: 2220-2239.Scraping batch: 116/953. authors: 2300-2319.Scraping batch: 120/953. authors: 2380-2399.Scraping batch: 121/953. authors: 2400-2419.Scraping batch: 131/953. authors: 2600-2619.Scraping batch: 143/953. authors: 2840-2859.Scraping batch: 144/953. authors: 2860-2879.Scraping batch: 146/953. authors: 2900-2919.Scraping batch: 153/953. authors: 3040-3059.Scraping batch: 159/953. authors: 3160-3179.\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SSLError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4478/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_tab\u001b[0;34m(s, url, selector)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4478/66039525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/nodelist_test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mauthor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(urls, items, n_pages, start_pos, batch_size, out_file)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape_urls\u001b[0;34m(s, urls, items)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m\"\"\"Wrapper to scrape a list of urls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(s, url, items)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'authors'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         result = await asyncio.gather(\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mscrape_author_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mscrape_author_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4478/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_page\u001b[0;34m(s, url, item)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0murl_dep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/researcherdepartaments.html?onlytab=true'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'table.table tr td'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0minstitution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_author_tab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_dep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4478/2440049347.py\u001b[0m in \u001b[0;36mscrape_author_tab\u001b[0;34m(s, url, selector)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed request to url: {url}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SSLError' is not defined"
     ]
    }
   ],
   "source": [
    "# Get author data in batch\n",
    "batch_size=20\n",
    "out_file = './data/nodelist_test.csv'\n",
    "\n",
    "author_data = await scrape(urls, items='authors', batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd803dc5-ca2b-4d2b-8db4-63fa48867dac",
   "metadata": {},
   "source": [
    "## Edgelist: scrape publications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e846e-310f-4b0e-afbc-7c32e0162817",
   "metadata": {},
   "source": [
    "### Get links to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ed63696-3202-4545-ba52-6bb207a2a122",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of pages to scrape.\n",
      "Scraping 2240 paper_links in 112 batches of 20.\n",
      "Saving results to ./data/paper_links_test.csv.\n",
      "Last batch: 36.81 seconds. Estimated time left: 0h57m3s.  Scraping batch: 7/112. paper_links: 120-139.Scraping batch: 8/112. paper_links: 140-159.Scraping batch: 9/112. paper_links: 160-179.Scraping batch: 10/112. paper_links: 180-199.Scraping batch: 11/112. paper_links: 200-219.Scraping batch: 12/112. paper_links: 220-239.Scraping batch: 13/112. paper_links: 240-259.Scraping batch: 14/112. paper_links: 260-279.Scraping batch: 15/112. paper_links: 280-299.Scraping batch: 16/112. paper_links: 300-319.Scraping batch: 17/112. paper_links: 320-339.Scraping batch: 18/112. paper_links: 340-359.Scraping batch: 19/112. paper_links: 360-379.Scraping batch: 20/112. paper_links: 380-399.Scraping batch: 21/112. paper_links: 400-419.\r"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape_url\u001b[0;34m(s, url, items)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div.panel.panel-info table.table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4478/2065619636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mout_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/paper_links_test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpaper_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'paper_links'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(urls, items, n_pages, start_pos, batch_size, out_file)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4478/2050874392.py\u001b[0m in \u001b[0;36mscrape_urls\u001b[0;34m(s, urls, items)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m\"\"\"Wrapper to scrape a list of urls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscrape_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get links to papers in batch\n",
    "batch_size=20\n",
    "out_file = './data/paper_links_test.csv'\n",
    "\n",
    "paper_links = await scrape(items='paper_links', batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376263f1-2ef7-4157-b82f-e0e7bfa89087",
   "metadata": {},
   "source": [
    "### Get coauthors in paper pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30b0d4b4-48c2-4f62-8733-d922a9b8cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build urls\n",
    "paper_urls = pd.read_csv('./data/paper_links.csv')\n",
    "paper_urls = list(paper_urls['0'])\n",
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "urls = [url_root + url for url in paper_urls]\n",
    "\n",
    "# Run in batch\n",
    "batch_size = 20\n",
    "out_file = './data/coauthors.csv'\n",
    "\n",
    "papers = await scrape(urls=urls, items='papers', batch_size=batch_size, out_file=out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f5c9f0-47a2-443f-a947-c4aa87218e35",
   "metadata": {},
   "source": [
    "# EXTRA CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c403a7-a638-46b2-8bdd-abe3a0682f02",
   "metadata": {},
   "source": [
    "## Speed Test: Sync vs Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c79c9-ca6c-4920-a254-78aefaf90948",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=crisrp&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Researchers&sort_by=crisrp.fullName_sort&order=asc&rpp=300&etal=0&start='\n",
    "\n",
    "urls = [url_root + str(page*100) for page in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c23e1-d21d-49ce-977c-43650099bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "def get_author_links_sync(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "    \n",
    "def main_sync(urls):\n",
    "    s = HTMLSession()\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        rows = get_author_links_sync(s, url)\n",
    "        result.append(rows)\n",
    "    return rows\n",
    "        \n",
    "async def get_author_links(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = await s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "\n",
    "async def main(urls):\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (get_author_links(s, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "        \n",
    "t1 = time.perf_counter()\n",
    "result = await main(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Async: {t2-t1:.2f} seconds.)\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "result_sync = main_sync(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Sync: {t2-t1:.2f} seconds.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
