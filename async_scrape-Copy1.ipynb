{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93360b1b-2db1-41e5-81f2-128839c6635f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scraping Portal de la Reserca\n",
    "\n",
    "This notebook asynchrnously scrapes information in Portal de la Reserca. It can download the following items:\n",
    "\n",
    "- Links to author portals\n",
    "- Author information from the author portals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3fb252-e4cc-44f8-93f1-5cd99d578ee1",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb2abea5-ed33-43fa-819e-caaaa1e05ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a8bf0-ed83-450f-8ef7-48d4540e1c25",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e791e31-5a64-4f35-8d7d-a54114b3964d",
   "metadata": {},
   "source": [
    "## Helper Functions for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0623840-a0bb-4fa0-9045-7a5177cccff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def scrape_url(s, url, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async scrape URL. \n",
    "    Items = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "    \"\"\"\n",
    "    # print(f\"Scraping url: {url}\")\n",
    "    r = await s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    try:\n",
    "        rows = table.find('tr')\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for row in rows:\n",
    "        # Get columns in row\n",
    "        columns = row.find('td')\n",
    "        # Skip if empty row\n",
    "        if len(columns) == 0:\n",
    "            continue\n",
    "            \n",
    "        if items == 'paper_links':\n",
    "            # Get paper link\n",
    "            paper_link = columns[1].find('a')[0].attrs['href']\n",
    "            scrape_item = paper_link\n",
    "            \n",
    "        elif items == 'author_links':\n",
    "            # Get paper link\n",
    "            author_link = columns[0].find('a')[0].attrs['href']\n",
    "            scrape_item = author_link\n",
    "            \n",
    "        elif items == 'papers':\n",
    "            # Get paper data\n",
    "            paper_date    = columns[0].text\n",
    "            paper_title   = columns[1].text\n",
    "            paper_authors = columns[2].text\n",
    "            paper_type    = columns[3].text\n",
    "\n",
    "            paper = {}\n",
    "            paper['date'] = paper_date\n",
    "            paper['title'] = paper_title\n",
    "            paper['type'] = paper_type\n",
    "\n",
    "            paper_authors_list= paper_authors.split(';')\n",
    "            for i, author in enumerate(paper_authors_list):\n",
    "                paper[f\"author_{i}\"] = author\n",
    "\n",
    "            scrape_item = paper\n",
    "\n",
    "        elif items == 'authors':\n",
    "            # Get author data\n",
    "            author_name = columns[0].text\n",
    "            author_last = author_name.split(',')[0]\n",
    "\n",
    "            try:\n",
    "                author_first = author_name.split(',')[1]\n",
    "            except IndexError:  # If there is no comma in the name\n",
    "                author_first = ''\n",
    "\n",
    "            author_inst = columns[1].text\n",
    "\n",
    "            author_dict = {\n",
    "                'Last Name': author_last,\n",
    "                'First Name': author_first,\n",
    "                'Institution': author_inst,\n",
    "                }\n",
    "\n",
    "            scrape_item = author_dict\n",
    "            \n",
    "        # Append to paper links list\n",
    "        result_list.append(scrape_item)\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "\n",
    "async def main(urls, items='author_links'):\n",
    "    \"\"\"\n",
    "    Async main loop. Run withn await main(urls) in Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (scrape_url(s, url, items) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "def get_max_pages(url):\n",
    "    \"\"\"\n",
    "    Get max pages from pagination box in footer.\n",
    "    \"\"\"\n",
    "    session = HTMLSession()\n",
    "    r = session.get(url)\n",
    "    pagination_box = r.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)\n",
    "\n",
    "async def scrape(items='author_links', start_page=0, n_pages=None):\n",
    "    \"\"\"\n",
    "    Scrape Portal de la Reserca.\n",
    "    Options:\n",
    "        items   = ['paper_links', 'author_links', 'papers', 'authors']\n",
    "        n_pages = Number of pages to scrape.\n",
    "    \"\"\"\n",
    "    print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "    \n",
    "    # Url root for authors\n",
    "    if items == 'author_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=crisrp'                              + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Researchers'               + \\\n",
    "            '&sort_by=crisrp.fullName_sort'                 + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start='\n",
    "\n",
    "    elif items == 'paper_links':\n",
    "        url_root =                                            \\\n",
    "            'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "            'query='                                        + \\\n",
    "            '&location=publications'                        + \\\n",
    "            '&filter_field_1=resourcetype'                  + \\\n",
    "                '&filter_type_1=equals'                     + \\\n",
    "                '&filter_value_1=Items'                     + \\\n",
    "            '&filter_field_2=itemtype'                      + \\\n",
    "                '&filter_type_2=notequals'                  + \\\n",
    "                '&filter_value_2=Phd+Thesis'                + \\\n",
    "            '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "                '&order=asc'                                + \\\n",
    "            '&rpp=300'                                      + \\\n",
    "            '&etal=0'                                       + \\\n",
    "            '&start=' \n",
    "    \n",
    "    if not n_pages:\n",
    "        print(\"Calculating number of pages to scrape.\")\n",
    "        max_pages = get_max_pages(url_root + '0')\n",
    "        n_pages = max_pages - start_page\n",
    "\n",
    "    urls = [url_root + str(page*300) for page in range(start_page, start_page+n_pages)]\n",
    "\n",
    "    # items_to_scrape = 'author_links'\n",
    "\n",
    "    print(f\"Scraping {len(urls)} URLs starting in page {start_page}...\")\n",
    "    t1 = time.perf_counter()\n",
    "    result = await main(urls, items=items)\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    # Gather all results into single list\n",
    "    full_list = [href for sublist in result for href in sublist]\n",
    "    \n",
    "    print(f\"Scraped {len(full_list)} items in {t2-t1:.2f} seconds.\")\n",
    "    \n",
    "    return full_list\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b8973-a368-4e42-b12f-a8efea8add6d",
   "metadata": {},
   "source": [
    "## Helper functions for items in links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d28ed5c6-1e9e-4d08-95fb-eb1ed7c0d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_author_tab(s, url, selector):\n",
    "    r = await s.get(url)\n",
    "    result = r.html.find(selector)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "async def scrape_author_page(s, url, item='name'):\n",
    "    if item == 'name':\n",
    "        selector = 'div#fullNameDiv span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        result = result[0].text\n",
    "    \n",
    "    elif item == 'id':\n",
    "        selector = 'div#orcidDiv a span'\n",
    "        result = await scrape_author_tab(s, url, selector)\n",
    "        result = result[0].text\n",
    "    \n",
    "    elif item == 'institution':\n",
    "        url_dep = url + '/researcherdepartaments.html?onlytab=true'\n",
    "        selector = 'table.table tr td'\n",
    "        institution = await scrape_author_tab(s, url_dep, selector)\n",
    "        result = {}\n",
    "        try:\n",
    "            result['department'] = institution[0].text\n",
    "            result['institution'] = institution[1].text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    elif item == 'projects':\n",
    "        url_proj = url + '/publicresearcherprojects.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        projects = await scrape_author_tab(s, url_proj, selector)\n",
    "        project_list = []\n",
    "        for i in range(1,len(projects)):\n",
    "            project = projects[i].find('td a')[0].attrs['href']\n",
    "            project_list.append(project)\n",
    "        result = project_list\n",
    "    \n",
    "    elif item == 'groups':\n",
    "        url_group = url + '/orgs.html?onlytab=true'\n",
    "        selector = 'table.table tr'\n",
    "        groups = await scrape_author_tab(s, url_group, selector)\n",
    "        group_list = []\n",
    "        for i in range(1,len(groups)):\n",
    "            group = groups[i].find('td a')[0].attrs['href']\n",
    "            group_list.append(group)\n",
    "        result = group_list\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "async def scrape_author(s, url):\n",
    "    result = await asyncio.gather(\n",
    "            scrape_author_page(s, url, 'name'),\n",
    "            scrape_author_page(s, url, 'id'),\n",
    "            scrape_author_page(s, url, 'institution'),\n",
    "            scrape_author_page(s, url, 'projects'),\n",
    "            scrape_author_page(s, url, 'groups')\n",
    "        )\n",
    "    \n",
    "    author = {}\n",
    "    author['name'] = result[0]\n",
    "    author['id'] = result[1]\n",
    "    try:\n",
    "        author['department'] = result[2]['department']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        author['institution'] = result[2]['institution']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    author['projects'] = result[3]\n",
    "    author['groups'] = result[4]\n",
    "    \n",
    "    return author\n",
    "        \n",
    "    \n",
    "async def scrape_authors(urls):\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (scrape_author(s, url) for url in urls)\n",
    "    \n",
    "    return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39a41e-7dd5-432b-88c3-598abd0d1e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scrape authors to create Nodelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135df11e-5c71-4611-86a5-33a83e7c4da9",
   "metadata": {},
   "source": [
    "## Scrape links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756ece2-9cac-47a4-a9b8-142645b9884e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_root = 'https://portalrecerca.csuc.cat'\n",
    "\n",
    "# Get links to author pages\n",
    "author_urls = await scrape('author_links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804c31d-3924-433c-99a7-fd01e4cc7fdb",
   "metadata": {},
   "source": [
    "## Scrape items within links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9e9d7-4831-4f82-987b-96b00bc50922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape in batches to avoid being blocked\n",
    "batch_size = 100\n",
    "\n",
    "# How many authors to scrape\n",
    "n_authors = len(author_urls)\n",
    "\n",
    "print(f\"Scraping {n_authors} authors in batches of {batch_size}.\")\n",
    "\n",
    "result = []\n",
    "result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "\n",
    "for batch_start in range(0, n_authors, batch_size):\n",
    "    print(f\"Scraping batch {batch_start/batch_size+1:.0f}/{n_authors/batch_size:.0f}.\", end=\"\\r\")\n",
    "    try:\n",
    "        batch_urls = author_urls[batch_start:batch_start+batch_size]\n",
    "    except IndexError:\n",
    "        batch_urls = author_urls[batch_start:n_authors+1]\n",
    "        \n",
    "    urls = [url_root + url for url in batch_urls]\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    author_result = await scrape_authors(urls)\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print(f\"Last batch: {t2-t1:.2f} seconds.\", end=\" \")\n",
    "    \n",
    "    result.extend(author_result)\n",
    "    result_df = result_df.append(author_result, ignore_index=True)\n",
    "    result_df.to_csv('nodelist.csv')\n",
    "    \n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727755db-f04c-4515-b45a-a5f704664b21",
   "metadata": {},
   "source": [
    "# Scrape papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99109510-7a0f-4115-893a-cefbc08e61c2",
   "metadata": {},
   "source": [
    "## Scrape links to papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4f27dfc-b965-470c-bdbb-196cd6974f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chunks function\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Creat n-sized chunks of list lst\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "        \n",
    "mylist = list(range(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf8c7955-6cbf-4cfe-8d18-007c49942770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "[mylist[i:i+n] for i in range(0,len(mylist),n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f4cf5ac-c8cf-43a1-ad7d-580bc3ff8b70",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2382/1528118617.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next(chunks(mylist, 3))\n",
    "next(next(chunks(mylist, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77dbc141-767c-4c5c-939f-24c528a0304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root =                                            \\\n",
    "    'https://portalrecerca.csuc.cat/simple-search?' + \\\n",
    "    'query='                                        + \\\n",
    "    '&location=publications'                        + \\\n",
    "    '&filter_field_1=resourcetype'                  + \\\n",
    "        '&filter_type_1=equals'                     + \\\n",
    "        '&filter_value_1=Items'                     + \\\n",
    "    '&filter_field_2=itemtype'                      + \\\n",
    "        '&filter_type_2=notequals'                  + \\\n",
    "        '&filter_value_2=Phd+Thesis'                + \\\n",
    "    '&sort_by=dc.contributor.authors_sort'          + \\\n",
    "        '&order=asc'                                + \\\n",
    "    '&rpp=300'                                      + \\\n",
    "    '&etal=0'                                       + \\\n",
    "    '&start=' \n",
    "\n",
    "# print(\"Calculating number of pages to scrape.\")\n",
    "max_pages = 2240\n",
    "start_page = 0\n",
    "n_pages = 2240\n",
    "# max_pages = get_max_pages(url_root + '0')\n",
    "# n_pages = max_pages - start_page\n",
    "\n",
    "urls = [url_root + str(page*300) for page in range(start_page, start_page+n_pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa78961c-2264-466f-b32e-32509ca9ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://portalrecerca.csuc.cat/simple-search?query=&location=publications&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Items&filter_field_2=itemtype&filter_type_2=notequals&filter_value_2=Phd+Thesis&sort_by=dc.contributor.authors_sort&order=asc&rpp=300&etal=0&start='"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bf5fae6-f0f4-472f-be46-f9933f3cb399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paper_urls = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3f4af-0841-46e0-a561-c30b4cac0b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 2240 authors in batches of 100.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 0...\n",
      "Scraped 22200 items in 65.35 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 100...\n",
      "Scraped 6600 items in 46.25 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 200...\n",
      "Scraped 24000 items in 83.03 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 300...\n",
      "Scraped 16800 items in 73.40 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 400...\n",
      "Scraped 17700 items in 87.59 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 500...\n",
      "Scraped 22500 items in 99.69 seconds.\n",
      "Scraping paper_links from Portal de la Reserca.\n",
      "Scraping 100 URLs starting in page 600...\n"
     ]
    }
   ],
   "source": [
    "# Scrape in batches to avoid being blocked\n",
    "batch_size = 100\n",
    "\n",
    "# How many authors to scrape\n",
    "# n_pages = len(author_urls)\n",
    "n_pages = 2240\n",
    "\n",
    "print(f\"Scraping {n_pages} authors in batches of {batch_size}.\")\n",
    "\n",
    "result = []\n",
    "# result_df = pd.DataFrame(columns=['name', 'id', 'department', 'institution', 'projects', 'groups'])\n",
    "\n",
    "for batch_start in range(0, n_pages, batch_size):\n",
    "    print(f\"Scraping batch {batch_start/batch_size+1:.0f}/{n_pages/batch_size:.0f}.\", end=\"\\r\")\n",
    "    # try:\n",
    "        # batch_urls = paper_urls[batch_start:batch_start+batch_size]\n",
    "    # except IndexError:\n",
    "        # batch_urls = paper_urls[batch_start:n_pages+1]\n",
    "        \n",
    "    # urls = batch_urls\n",
    "    # urls = [url_root + url for url in batch_urls]\n",
    "    \n",
    "    t1 = time.perf_counter()\n",
    "    paper_links = await scrape('paper_links', n_pages=batch_size, start_page=batch_start)\n",
    "    # author_result = await scrape_authors(urls)\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    print(f\"Last batch: {t2-t1:.2f} seconds.\", end=\" \")\n",
    "    \n",
    "    result.extend(paper_links)\n",
    "    # result_df = result_df.append(author_result, ignore_index=True)\n",
    "    # result_df.to_csv('nodelist.csv')\n",
    "    \n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07dbf29-7960-43af-ae4c-1d05cd186c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result of last scrape:\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd24d92-0be8-46b4-913b-4cd8defd30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "pd.DataFrame(result).to_csv('data/paper_links_last.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1c7c9-af25-4378-99f4-115b94427946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_root = 'https://portalrecerca.csuc.cat'\n",
    "\n",
    "# Get links to author pages (Takes 2m to run)\n",
    "paper_links = await scrape('paper_links', n_pages=1000, start_page=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219edfdf-001e-4fe0-a465-33a13d737d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paper_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6f8a1-b050-4600-af4b-e6e05e7edb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output\n",
    "paper_links_df = pd.DataFrame(paper_links, columns=['paper_links'])\n",
    "paper_links_df.to_csv('./data/paper_links_100-199.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66261e2d-7095-4b01-9ab0-870577f1ca4e",
   "metadata": {},
   "source": [
    "## Scrape items within links to papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c403a7-a638-46b2-8bdd-abe3a0682f02",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Speed Test: Sync vs Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c79c9-ca6c-4920-a254-78aefaf90948",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=crisrp&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Researchers&sort_by=crisrp.fullName_sort&order=asc&rpp=300&etal=0&start='\n",
    "\n",
    "urls = [url_root + str(page*100) for page in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c23e1-d21d-49ce-977c-43650099bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "def get_author_links_sync(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "    \n",
    "def main_sync(urls):\n",
    "    s = HTMLSession()\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        rows = get_author_links_sync(s, url)\n",
    "        result.append(rows)\n",
    "    return rows\n",
    "        \n",
    "async def get_author_links(s, url):\n",
    "    print(f\"Getting url: {url}\")\n",
    "    r = await s.get(url)\n",
    "    table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "    rows = table.find('tr')\n",
    "    return rows\n",
    "\n",
    "async def main(urls):\n",
    "    s = AsyncHTMLSession()\n",
    "    tasks = (get_author_links(s, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "        \n",
    "t1 = time.perf_counter()\n",
    "result = await main(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Async: {t2-t1:.2f} seconds.)\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "result_sync = main_sync(urls)\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Sync: {t2-t1:.2f} seconds.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e4db0-eff9-407e-80fd-e9d531036f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EXTRA CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762926d7-bd0c-4347-a3b2-545ac114c595",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c76c7-dba7-49fe-b1e0-e220e8eafec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max pages from pagination box in footer\n",
    "def get_max_pages(session_object):\n",
    "    pagination_box = session_object.html.find('ul.pagination.pull-right')\n",
    "    pagination_items = pagination_box[0].find('li')\n",
    "    max_pages = pagination_items[-2].text.replace('.','').replace(',','')\n",
    "    return int(max_pages)\n",
    "\n",
    "# Get papers links\n",
    "def scrape( \n",
    "    items      = 'links', \n",
    "    url_root   = None,\n",
    "    max_pages  = None, \n",
    "    page_start = 0):\n",
    "    \"\"\"\n",
    "    Scrape Portal de la Reserca\n",
    "    Options:\n",
    "    - items = [links, papers, authors]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Scraping {items} from Portal de la Reserca.\")\n",
    "    \n",
    "    if not url_root:\n",
    "        if items == 'authors':\n",
    "            url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=crisrp&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Researchers&sort_by=crisrp.fullName_sort&order=asc&rpp=300&etal=0&start='\n",
    "        elif items == 'links' or items == 'papers':\n",
    "            url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=publications&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Items&filter_field_2=itemtype&filter_type_2=notequals&filter_value_2=Phd+Thesis&sort_by=dc.contributor.authors_sort&order=asc&rpp=300&etal=0&start=' \n",
    "            \n",
    "    session = HTMLSession()\n",
    "    \n",
    "    if not max_pages:\n",
    "        url = url_root + '0'\n",
    "        r = session.get(url)\n",
    "        max_pages = get_max_pages(r)\n",
    "            \n",
    "    page_number = page_start\n",
    "    \n",
    "    result_list = []\n",
    "        \n",
    "    while page_number < max_pages:\n",
    "        # Progress\n",
    "        print(f\"Progress: {round(page_number/(max_pages)*100)}%. Scraping page: {page_number}/{max_pages}.\", end=\"\\r\")\n",
    "\n",
    "        # Create URL\n",
    "        url = url_root + str(300*page_number)\n",
    "        # Update page counter\n",
    "        page_number += 1\n",
    "        # Get page\n",
    "        r = session.get(url)\n",
    "        # Get table\n",
    "        table = r.html.find('div.panel.panel-info table.table', first=True)\n",
    "        # Get rows from table\n",
    "        rows = table.find('tr')\n",
    "\n",
    "        for row in rows:\n",
    "            # Get columns in row\n",
    "            columns = row.find('td')\n",
    "            # Skip if empty row\n",
    "            if len(columns) == 0:\n",
    "                continue\n",
    "                \n",
    "            if items == 'links':\n",
    "                # Get paper link\n",
    "                paper_link = columns[1].find('a')[0].attrs['href']\n",
    "                scrape_item = paper_link\n",
    "            \n",
    "            elif items == 'papers':\n",
    "                # Get paper data\n",
    "                paper_date    = columns[0].text\n",
    "                paper_title   = columns[1].text\n",
    "                paper_authors = columns[2].text\n",
    "                paper_type    = columns[3].text\n",
    "\n",
    "                paper = {}\n",
    "                paper['date'] = paper_date\n",
    "                paper['title'] = paper_title\n",
    "                paper['type'] = paper_type\n",
    "\n",
    "                paper_authors_list= paper_authors.split(';')\n",
    "                for i, author in enumerate(paper_authors_list):\n",
    "                    paper[f\"author_{i}\"] = author\n",
    "\n",
    "                scrape_item = paper\n",
    "                \n",
    "            elif items == 'authors':\n",
    "                # Get author data\n",
    "                author_name = columns[0].text\n",
    "                author_last = author_name.split(',')[0]\n",
    "\n",
    "                try:\n",
    "                    author_first = author_name.split(',')[1]\n",
    "                except IndexError:  # If there is no comma in the name\n",
    "                    author_first = ''\n",
    "\n",
    "                author_inst = columns[1].text\n",
    "\n",
    "                author_dict = {\n",
    "                    'Last Name': author_last,\n",
    "                    'First Name': author_first,\n",
    "                    'Institution': author_inst,\n",
    "                    }\n",
    "\n",
    "                scrape_item = author_dict\n",
    "                \n",
    "            # Append to paper links list\n",
    "            result_list.append(scrape_item)\n",
    "        \n",
    "    if page_number == max_pages:\n",
    "        print(\"Progress: 100%                                  \")\n",
    "        \n",
    "    if items == 'links':\n",
    "        result_df = pd.DataFrame(result_list, columns=['paper links'])\n",
    "    else:\n",
    "        result_df = pd.DataFrame.from_records(result_list)\n",
    "              \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37981174-29c7-4b49-9c62-122fb243b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coauthors(paper_links_df, n_papers=None, start=0):\n",
    "    \"\"\"\n",
    "    Get coauthors list from list of paper links\n",
    "    Input:\n",
    "    - paper_links_df: Dataframe with hyperlinks to papers.\n",
    "    - n_papers: number of papers to scrape.\n",
    "    - start: initial paper to start scraping.\n",
    "    \"\"\"\n",
    "    if not n_papers:\n",
    "        n_papers =  len(paper_links_df) - start\n",
    "    print(f\"Scraping author ID's of papers in Portal de la Reserca. Starting in paper {start} and scraping {n_papers} papers.\")\n",
    "    \n",
    "    session = HTMLSession()\n",
    "    \n",
    "    # Get author links\n",
    "    result_list = []\n",
    "    for i, link in enumerate(paper_links_df['paper links']):\n",
    "        \n",
    "        if i < start:\n",
    "            continue\n",
    "            \n",
    "        threshold = start + n_papers\n",
    "        if i >= threshold:\n",
    "            break\n",
    "            \n",
    "        print(f\"Progress: {round(i/(threshold)*100)}%. Scraping paper: {i+1}/{threshold}.\", end=\"\\r\")\n",
    "        url_root = 'https://portalrecerca.csuc.cat'\n",
    "        url = url_root + link\n",
    "        r = session.get(url)\n",
    "        # print(url, end=\"\\r\")\n",
    "        # print(r.text, end=\"\\r\")\n",
    "        try:\n",
    "            table = r.html.find('table.itemDisplayTable')[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        rows = table.find('tr')\n",
    "        # Get links to authors\n",
    "        author_links = rows[2].find('td')[1].find('a.author')\n",
    "        author_hrefs = []\n",
    "        for i, link in enumerate(author_links):\n",
    "            href = link.attrs['href']\n",
    "            author_hrefs.append(href)\n",
    "\n",
    "        # Visit author links and get id\n",
    "        author_list = []\n",
    "        for href in author_hrefs:\n",
    "            url = url_root + href\n",
    "            author_page = session.get(url).html\n",
    "            author_id = author_page.find('div#orcidDiv span')[0].text\n",
    "            author_list.append(author_id)\n",
    "\n",
    "            # author_name = author_page.find('div#fullNameDiv span')[0].text\n",
    "            # author = {'name':author_name, 'id': author_id}\n",
    "            # author_list.append(author)\n",
    "\n",
    "        result_list.append(author_list)\n",
    "        \n",
    "    print(f\"Progress: 100%.                          \")\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cfd93-7e5d-482e-a4bb-cb81948a73e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scrape paper links, paper authors, and author information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98967e-7c01-4cba-aded-7a8e06836360",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864adf4e-b7c6-4c97-975a-fdeb08e2c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "\n",
    "# Choose custom URLs\n",
    "# url = 'https://portalrecerca.csuc.cat/simple-search?filtername=resourcetype&filterquery=Researchers&filtertype=equals&sort_by=crisrp.fullName_sort&order=ASC&location=crisrp'\n",
    "# url_root = 'https://portalrecerca.csuc.cat/simple-search?query=&location=crisrp&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Researchers&sort_by=crisrp.fullName_sort&order=asc&rpp=100&etal=0&start='\n",
    "# Download 300 papers, ordered by author name\n",
    "# url_root = \\\n",
    "#     'https://portalrecerca.csuc.cat/simple-search?query=&location=publications&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Items&filter_field_2=itemtype&filter_type_2=notequals&filter_value_2=Phd+Thesis&sort_by=dc.contributor.authors_sort&order=asc&rpp=300&etal=0&start='\n",
    "# # Download IGTP papers\n",
    "# url_root = \\\n",
    "#     'https://portalrecerca.csuc.cat/simple-search?query=&location=publications&filter_field_1=resourcetype&filter_type_1=equals&filter_value_1=Items&filter_field_2=itemtype&filter_type_2=notequals&filter_value_2=Phd+Thesis&filter_field_3=location.coll&filter_type_3=equals&filter_value_3=349&sort_by=dc.contributor.authors_sort&order=asc&rpp=300&etal=0&start='"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632d640-6222-4945-a3d4-be7238e6499b",
   "metadata": {},
   "source": [
    "## Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5cae7-476f-4b53-97f4-999e7f52be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_to_scrape = 'papers'\n",
    "items_to_scrape = 'links'\n",
    "result_df = scrape(items_to_scrape, max_pages=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730c9219-8d8b-4e8b-a0f1-0994f83e34c9",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407105a-81d9-4ccf-ace0-73d3796b5da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_folder =  '../data/network of talent/research networks/Scraping/temp/'\n",
    "result_df.to_csv(output_folder + items_to_scrape + '.csv', index=False)\n",
    "print('')\n",
    "print(f\"Saved output to file {items_to_scrape}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e459cf4-d9b2-4f6c-a190-1c6d2a05348b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Scrape paper links to get coauthors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b91df-0f9a-4e80-8185-80dbbb47e642",
   "metadata": {},
   "source": [
    "## Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142a9c7-b9df-4cb4-8a57-be40e2dd75c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get links to papers\n",
    "paper_links_df = scrape('links')\n",
    "\n",
    "# data_folder =  '../data/network of talent/research networks/Scraping/'\n",
    "# paper_links_df = pd.read_csv(data_folder + 'temp/links.csv')\n",
    "\n",
    "# Get coauthor IDs from paper links\n",
    "result_list = get_coauthors(paper_links_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e1010-6669-4362-a75b-47576f144308",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151cff-f605-4e42-8061-022b4a88c796",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415d097-76ab-41ac-8183-4930a61227d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "result_df = pd.DataFrame.from_records(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680b0fc-40b7-44d1-813e-b0ab703bca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d7983-2e7d-4069-a470-eeafbf4079c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "data_folder =  '../data/network of talent/research networks/Scraping/'\n",
    "result_df.to_csv(data_folder + 'processed/coauthors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007f563-e3a8-4cb2-9e7f-a6b1f40f4d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17f01e-3157-4fd9-9e5e-b5881c66ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d80be0-e193-461f-bcaa-e979d0935c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb0650-033e-46d6-9d82-06ed16d0af20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Todo\n",
    "1. Scrape publications list and get ids of authors. Each row should be a list of ids.\n",
    "2. Check results from scraping ids of authors in publications (coauthors.csv)\n",
    "2. Create matrix or edgelist or coauthors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527293c-e445-4dea-a937-4b0c9a6ec2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nodelist\n",
    "\n",
    "1. Get all authors links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea834d-7dfe-44b4-8c43-030096a2e100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "url = 'https://portalrecerca.csuc.cat/orcid/0000-0003-0763-2695'\n",
    "r = session.get(url)\n",
    "r.html.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
