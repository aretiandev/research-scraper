{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9722fea-7375-40cd-be0d-91abd54100a6",
   "metadata": {},
   "source": [
    "# Post processing scraped data\n",
    "\n",
    "This notebook processes the scraped data from Portal de la Reserca to create the Nodelist and Edgelist to plot in Gephi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463cb19-4f60-4644-811a-db88b373364d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automated Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15fe3e8-0142-4872-867a-51b2d2b3558f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be02552d-aa30-455f-a12c-c53217ddbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(x):\n",
    "    \"\"\"Convert string column to list\"\"\"\n",
    "    try:\n",
    "        result = literal_eval(x)\n",
    "    except ValueError:\n",
    "        result = np.nan\n",
    "    return result\n",
    "\n",
    "def belongs_to_list(row, df):\n",
    "    try:\n",
    "        result = bool(set(row['orcids']) & set(df))\n",
    "    except TypeError:\n",
    "        result = False\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381627c-a51d-43aa-9182-0989dd24aa47",
   "metadata": {},
   "source": [
    "## Define main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b9d55-3122-43a8-b022-9696dee21a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edgelist\n",
    "papers_df = pd.read_csv('./data/papers.csv')\n",
    "res_df = pd.read_csv('./data/nodelist.csv')\n",
    "\n",
    "def calculate_collaborations(institution, papers_df, res_df, save=False, threshold=None):\n",
    "\n",
    "    # Filter researchers\n",
    "    res_df_inst = res_df.loc[res_df['institution'] == institution]\n",
    "    res_inst = res_df_inst['id'].unique()\n",
    "\n",
    "    # Convert strings to list of coauthors\n",
    "    papers_df['orcids'] = papers_df['orcids'].apply(lambda x: convert_to_list(x))\n",
    "\n",
    "    # Identify authors in institution\n",
    "    institution_df = res_inst\n",
    "    mask = papers_df.apply(lambda x: belongs_to_list(x, institution_df), axis=1)\n",
    "\n",
    "    selected_df = papers_df[mask]\n",
    "    \n",
    "    # Test with n papers for debugging (max 2800)\n",
    "    if threshold:\n",
    "        selected_df = selected_df[:threshold]\n",
    "\n",
    "    # Get papers column\n",
    "    papers = selected_df['orcids'].copy()\n",
    "    papers = papers.reset_index(drop=True)\n",
    "\n",
    "    # Get unique list of authors from papers\n",
    "    authors_index = list(set(papers.sum()))\n",
    "\n",
    "    authors_index.sort()\n",
    "\n",
    "    # Create boolean matrix with papers\n",
    "    paper_bool_df = pd.DataFrame(columns=authors_index, index=range(len(papers)))\n",
    "\n",
    "    for i, paper in enumerate(papers):\n",
    "        paper_bool_df.loc[i,:] = 0\n",
    "        for orcid in paper:\n",
    "            paper_bool_df.loc[i,orcid] = 1\n",
    "\n",
    "    # Create papers matrix in numpy\n",
    "    papers_mat = paper_bool_df.to_numpy()\n",
    "\n",
    "    # Build collaboration vector to store results\n",
    "    n_authors = len(authors_index)\n",
    "    collabs_length = int(n_authors*(n_authors+1)/2 - n_authors) \n",
    "    collabs = np.zeros(shape=(collabs_length))\n",
    "\n",
    "    # Store copy of papers_mat for iterative updating\n",
    "    papers_mat_i = papers_mat\n",
    "\n",
    "    # Initialize writing position\n",
    "    start_pos = 0\n",
    "\n",
    "    for i in range(0, n_authors-1): #last author loop is unnecessary\n",
    "        print(f\"Progress: {i/(n_authors-2)*100:.0f}%.\", end=\"\\r\")\n",
    "\n",
    "        # Initialize matrix\n",
    "        C = np.identity(n_authors-i)\n",
    "        C = C[:,1:]\n",
    "        C[0] = 1\n",
    "\n",
    "        # Main inner product\n",
    "        result = np.dot(papers_mat_i, C)\n",
    "\n",
    "        # Calculate number of collaborations\n",
    "        result = result - 1\n",
    "        result = result.clip(0)\n",
    "        collabs_author = result.sum(axis=0)\n",
    "\n",
    "        # Store in collabs vector\n",
    "        end_pos = start_pos + n_authors - i - 1\n",
    "\n",
    "        collabs[start_pos:end_pos] = collabs_author\n",
    "\n",
    "        # Update start_pos for writing next loop\n",
    "        start_pos = end_pos \n",
    "\n",
    "        # Remove first author from papers_mat for next loop\n",
    "        papers_mat_i = papers_mat_i[:,1:]\n",
    "\n",
    "    author_combinations = combinations(authors_index,2)\n",
    "    collabs_df = pd.DataFrame(list(author_combinations), columns=['source', 'target'])\n",
    "    collabs_df['value']=collabs\n",
    "\n",
    "    if save:\n",
    "        collabs_df.to_csv(f'./data/edgelist_{institution}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627dcb7-49b2-44da-bdb5-280165222b8a",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41307be8-77a1-4b49-abcc-6e533bc33931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select institution\n",
    "institution_list = ['IGTP']\n",
    "\n",
    "for institution in institution_list:\n",
    "    calculate_collaborations(institution, papers_df, res_df, save=True, threshold=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
